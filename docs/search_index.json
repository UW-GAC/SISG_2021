[["index.html", "SISG 2021 Module 16: Computational Pipeline for WGS Data 1 Introduction 1.1 Schedule 1.2 R packages used 1.3 Resources", " SISG 2021 Module 16: Computational Pipeline for WGS Data 2021-07-20 1 Introduction This site contains course materials for SISG Module 16: Computational Pipeline for WGS Data, July 21-23, 2021. The official SISG web page can be found here (requires login). Lectures Slides for lectures are linked below in the schedule. All lectures will be given via Zoom, and recordings of lectures will be posted afterwards. Zoom Link for lectures: https://washington.zoom.us/j/92319788832 Tutorials and Exercises To work through the tutorials and exercises, log into NHLBI BioData Catalyst powered by Seven Bridges with your username and password – we will use this platform for all live demonstrations during the course. All of the R code and data can also be downloaded from the github repository from which the site is built and run on your local machine after the course has ended. Download the workshop data and exercises: https://github.com/UW-GAC/SISG_2021/archive/master.zip Slack Join the Slack channel here: https://uwbiostatisticssisg.slack.com 1.1 Schedule NOTE: All times are Pacific Daylight Time (GMT-07:00) Wednesday, July 21 Zoom session 11:30-13:15 PDT - Introduction and Genomic Data Storage Lecture Introduction (slides) (video coming soon) Using BioData Catalyst for SISG exercises (instructions) (video coming soon) Sequencing data formats (slides) (video coming soon) Intro to Genomic Data Structure (slides) (video coming soon) Interactive breakout rooms GDS format (Tutorial) (Exercise) (Solution) Discussion (video coming soon) Zoom session 13:45-14:30 PDT - Phenotype harmonization Lecture Phenotype harmonization (slides) (pre-recorded lecture) Interactive breakout rooms Harmonization in R (Tutorial) Discussion (video coming soon) Thursday, July 22 Zoom session 8:00-9:45 PDT - Association tests, Part I Lecture Association tests: Methods and motivation (slides) (video coming soon) GENESIS for association tests (slides) (video coming soon) Interactive breakout rooms Null model (Tutorial) (Exercise) (Solution) Single-variant association tests (Tutorial) (Exercise) (Solution) Discussion (video coming soon) Zoom session 10:15-11:45 PDT - Association tests, Part II Lecture Aggregate tests (slides) (video coming soon) Interactive breakout rooms Sliding window tests (Tutorial) (Exercise) (Solution) Discussion (video coming soon) Zoom session 12:45-14:30 PDT - Population Structure and Relatedness Lecture Population structure inference (slides) (video coming soon) Relatedness inference (slides) (video coming soon) R packages for ancestry and relatedness (slides) (video coming soon) Interactive breakout rooms Ancestry and relatedness inference (Tutorial) (Exercises) (Solutions) Discussion (video coming soon) Friday, July 23 Zoom session 8:00-9:30 PDT - Mixed models Lecture Mixed model association testing (slides) (video coming soon) Interactive breakout rooms Mixed models in R (Tutorial) (Exercise) (Solution) Discussion (video coming soon) Zoom session 9:45-11:30 PDT - Variant annotation Lecture Variant annotation (slides) (video coming soon) Interactive breakout rooms Association testing using variant annotation (Tutorial) (Exercise) (Solution) Discussion (video coming soon) Zoom session 12:30-13:45 PDT - Working in the cloud Lecture Analysis pipelines on the cloud (slides) (video coming soon) Running a workflow on BioData Catalyst (video coming soon) Interactive breakout rooms Running a GWAS workflow (Tutorial) Discussion (video coming soon) Zoom session 14:00-14:30 PDT - Open session for questions/advice Discussion (video coming soon) 1.2 R packages used GENESIS SeqArray SeqVarTools SNPRelate TopmedPipeline tidyverse GGally 1.3 Resources NHLBI BioData Catalyst Powered by Seven Bridges Getting Started Guide for SISG21 Module 16 If you are new to R, you might find the following material helpful: Introduction to R materials from SISG Module 3 Graphics with ggplot2 Data manipulation with dplyr "],["gds-format.html", "2 GDS format 2.1 Convert a VCF to GDS 2.2 Exploring a GDS file 2.3 Exercises 2.4 Solutions", " 2 GDS format GDS is Genomic Data Structure, a storage format that can efficiently store genomic data and provide fast random access to subsets of the data. For more information on GDS for sequence data, read the SeqArray package vignette. 2.1 Convert a VCF to GDS To use the R packages developed at the University of Washington Genetic Analysis Center for sequence data, we first need to convert a VCF file to GDS. (If the file is BCF, use https://samtools.github.io/bcftools/bcftools.html to convert to VCF.) library(SeqArray) repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) vcffile &lt;- &quot;data/1KG_phase3_subset_chr1.vcf.gz&quot; if (!file.exists(vcffile)) download.file(file.path(repo_path, vcffile), vcffile) gdsfile &lt;- &quot;data/1KG_phase3_subset_chr1.gds&quot; # convert the VCF to GDS seqVCF2GDS(vcffile, gdsfile, fmt.import=&quot;GT&quot;, storage.option=&quot;LZMA_RA&quot;) ## Tue Jul 20 17:39:19 2021 ## Variant Call Format (VCF) Import: ## file(s): ## 1KG_phase3_subset_chr1.vcf.gz (120.7K) ## file format: VCFv4.1 ## genome reference: ftp://ftp.1000genomes.ebi.ac.uk//vol1/ftp/technical/reference/phase2_reference_assembly_sequence/hs37d5.fa.gz ## the number of sets of chromosomes (ploidy): 2 ## the number of samples: 1,126 ## genotype storage: bit2 ## compression method: LZMA_RA ## # of samples: 1126 ## Output: ## data/1KG_phase3_subset_chr1.gds ## Parsing &#39;1KG_phase3_subset_chr1.vcf.gz&#39;: ## + genotype/data { Bit2 2x1126x1121 LZMA_ra(0.01%), 42B } ## Digests: ## sample.id [md5: 6b04799356320063276ad84f70713429] ## variant.id [md5: 35a6feaa59ff5659e2d16d2afb6923d4] ## position [md5: 868601c0a2f333d113be2040295602b4] ## chromosome [md5: 774667f087db893e987ce02dc9188ee2] ## allele [md5: bb36e628c2faf01dfc2792615b6c2350] ## genotype [md5: 59be1f80c30eee4a93b82077530eba87] ## phase [md5: ae6a90ef5cceae947052b0c8999a4a94] ## annotation/id [md5: 045802d7856c955f4710288e4764fcbe] ## annotation/qual [md5: d6b8ed9bd5cac7dd7898bb679675fc7c] ## annotation/filter [md5: 8294edaf60fffc1e9aa693b4bc423dd1] ## Done. ## Tue Jul 20 17:39:20 2021 ## Optimize the access efficiency ... ## Clean up the fragments of GDS file: ## open the file &#39;data/1KG_phase3_subset_chr1.gds&#39; (71.3K) ## # of fragments: 123 ## save to &#39;data/1KG_phase3_subset_chr1.gds.tmp&#39; ## rename &#39;data/1KG_phase3_subset_chr1.gds.tmp&#39; (70.5K, reduced: 744B) ## # of fragments: 61 ## Tue Jul 20 17:39:20 2021 2.2 Exploring a GDS file We can interact with the GDS file using the SeqArray package. # open a connection to the GDS file gds &lt;- seqOpen(gdsfile) gds ## Object of class &quot;SeqVarGDSClass&quot; ## File: /Users/mconomos/Documents/SISG_2021/data/1KG_phase3_subset_chr1.gds (70.5K) ## + [ ] * ## |--+ description [ ] * ## |--+ sample.id { Str8 1126 LZMA_ra(9.66%), 877B } * ## |--+ variant.id { Int32 1120 LZMA_ra(17.5%), 793B } * ## |--+ position { Int32 1120 LZMA_ra(78.5%), 3.4K } * ## |--+ chromosome { Str8 1120 LZMA_ra(4.55%), 109B } * ## |--+ allele { Str8 1120 LZMA_ra(26.0%), 1.2K } * ## |--+ genotype [ ] * ## | |--+ data { Bit2 2x1126x1121 LZMA_ra(8.34%), 51.4K } * ## | |--+ extra.index { Int32 3x0 LZMA_ra, 18B } * ## | \\--+ extra { Int16 0 LZMA_ra, 18B } ## |--+ phase [ ] ## | |--+ data { Bit1 1126x1120 LZMA_ra(0.11%), 177B } * ## | |--+ extra.index { Int32 3x0 LZMA_ra, 18B } * ## | \\--+ extra { Bit1 0 LZMA_ra, 18B } ## |--+ annotation [ ] ## | |--+ id { Str8 1120 LZMA_ra(40.4%), 3.6K } * ## | |--+ qual { Float32 1120 LZMA_ra(2.46%), 117B } * ## | |--+ filter { Int32,factor 1120 LZMA_ra(2.46%), 117B } * ## | |--+ info [ ] ## | \\--+ format [ ] ## \\--+ sample.annotation [ ] The seqGetData function is the basic function for reading in data from a GDS file # the unique sample identifier comes from the VCF header sample.id &lt;- seqGetData(gds, &quot;sample.id&quot;) length(sample.id) ## [1] 1126 head(sample.id) ## [1] &quot;HG00096&quot; &quot;HG00097&quot; &quot;HG00099&quot; &quot;HG00100&quot; &quot;HG00101&quot; &quot;HG00102&quot; # a unique integer ID is assigned to each variant variant.id &lt;- seqGetData(gds, &quot;variant.id&quot;) length(variant.id) ## [1] 1120 head(variant.id) ## [1] 1 2 3 4 5 6 chr &lt;- seqGetData(gds, &quot;chromosome&quot;) head(chr) ## [1] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; pos &lt;- seqGetData(gds, &quot;position&quot;) head(pos) ## [1] 970546 985900 1025045 1265550 1472676 1735725 id &lt;- seqGetData(gds, &quot;annotation/id&quot;) head(id) ## [1] &quot;&quot; &quot;rs17160776&quot; &quot;&quot; &quot;&quot; &quot;rs78293298&quot; &quot;&quot; There are additional useful functions for summary level data. # reference allele frequency of each variant afreq &lt;- seqAlleleFreq(gds) head(afreq) ## [1] 0.9960036 0.9507105 0.9995560 0.9991119 0.9928952 0.9977798 summary(afreq) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.9866 0.9987 0.9436 0.9996 1.0000 hist(afreq, breaks=50) We can define a filter on the gds object. After using the seqSetFilter command, all subsequent reads from the gds object are restricted to the selected subset of data, until a new filter is defined or seqResetFilter is called. seqSetFilter(gds, variant.id=91:100, sample.id=sample.id[1:5]) ## # of selected samples: 5 ## # of selected variants: 10 Genotype data is stored in a 3-dimensional array, where the first dimension is always length 2 for diploid genotypes. The second and third dimensions are samples and variants, respectively. The values of the array denote alleles: 0 is the reference allele and 1 is the alternate allele. For multiallelic variants, other alternate alleles are represented as integers &gt; 1. geno &lt;- seqGetData(gds, &quot;genotype&quot;) dim(geno) ## [1] 2 5 10 geno[,,1:2] ## , , 1 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 1 1 1 ## [2,] 1 1 1 1 1 ## ## , , 2 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 1 0 0 1 The SeqVarTools package has some additional functions for interacting with SeqArray-format GDS files. There are functions providing more intuitive ways to read in genotypes. library(SeqVarTools) # return genotypes in matrix format getGenotype(gds) ## variant ## sample 91 92 93 94 95 96 97 98 99 100 ## HG00096 &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00097 &quot;1|1&quot; &quot;0|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00099 &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00100 &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00101 &quot;1|1&quot; &quot;0|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; getGenotypeAlleles(gds) ## variant ## sample 91 92 93 94 95 96 97 98 99 100 ## HG00096 &quot;G|G&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;C|C&quot; &quot;T|T&quot; ## HG00097 &quot;G|G&quot; &quot;T|C&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|C&quot; &quot;C|C&quot; &quot;T|T&quot; ## HG00099 &quot;G|G&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|C&quot; &quot;C|C&quot; &quot;T|T&quot; ## HG00100 &quot;G|G&quot; &quot;T|T&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; ## HG00101 &quot;G|G&quot; &quot;T|C&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;C|C&quot; &quot;T|T&quot; refDosage(gds) ## variant ## sample 91 92 93 94 95 96 97 98 99 100 ## HG00096 0 2 2 2 2 2 2 0 2 2 ## HG00097 0 1 2 2 2 2 2 1 2 2 ## HG00099 0 2 2 2 2 2 2 1 2 2 ## HG00100 0 2 2 2 2 2 2 2 2 2 ## HG00101 0 1 2 2 2 2 2 2 2 2 altDosage(gds) ## variant ## sample 91 92 93 94 95 96 97 98 99 100 ## HG00096 2 0 0 0 0 0 0 2 0 0 ## HG00097 2 1 0 0 0 0 0 1 0 0 ## HG00099 2 0 0 0 0 0 0 1 0 0 ## HG00100 2 0 0 0 0 0 0 0 0 0 ## HG00101 2 1 0 0 0 0 0 0 0 0 There are functions to extract variant information. # look at reference and alternate alleles refChar(gds) ## [1] &quot;A&quot; &quot;T&quot; &quot;G&quot; &quot;G&quot; &quot;G&quot; &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;T&quot; altChar(gds) ## [1] &quot;G&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;G&quot; &quot;T&quot; &quot;G&quot; &quot;A&quot; &quot;C&quot; # data.frame of variant information variantInfo(gds) ## variant.id chr pos ref alt ## 1 91 1 15461365 A G ## 2 92 1 15817319 T C ## 3 93 1 16150840 G A ## 4 94 1 16272296 G A ## 5 95 1 16300899 G A ## 6 96 1 16353896 A G ## 7 97 1 16414772 C T ## 8 98 1 16446764 C G ## 9 99 1 16448267 C A ## 10 100 1 16699413 T C # reset the filter to all variants and samples seqResetFilter(gds) ## # of selected samples: 1,126 ## # of selected variants: 1,120 # how many alleles for each variant? n &lt;- seqNumAllele(gds) table(n) ## n ## 2 3 4 ## 1099 20 1 # some variants have more than one alternate allele multi.allelic &lt;- which(n &gt; 2) altChar(gds)[multi.allelic] ## [1] &quot;GT,G&quot; &quot;G,T&quot; &quot;A,T&quot; &quot;A,T&quot; &quot;ATG,ATGTG&quot; &quot;C,G&quot; &quot;A,T&quot; &quot;C,T&quot; &quot;A,C&quot; ## [10] &quot;TAA,T&quot; &quot;GTTA,GTTT&quot; &quot;GCC,GCCC,G&quot; &quot;A,C&quot; &quot;A,C&quot; &quot;A,C&quot; &quot;CAAGCAT,CGAGCAT&quot; &quot;CATTATT,C&quot; &quot;AT,C&quot; ## [19] &quot;TGTGA,C&quot; &quot;CCATT,CCATTCATT&quot; &quot;C,G&quot; # extract a particular alternate allele # first alternate altChar(gds, n=1)[multi.allelic] ## [1] &quot;GT&quot; &quot;G&quot; &quot;A&quot; &quot;A&quot; &quot;ATG&quot; &quot;C&quot; &quot;A&quot; &quot;C&quot; &quot;A&quot; &quot;TAA&quot; &quot;GTTA&quot; &quot;GCC&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;CAAGCAT&quot; ## [17] &quot;CATTATT&quot; &quot;AT&quot; &quot;TGTGA&quot; &quot;CCATT&quot; &quot;C&quot; # second alternate altChar(gds, n=2)[multi.allelic] ## [1] &quot;G&quot; &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;ATGTG&quot; &quot;G&quot; &quot;T&quot; &quot;T&quot; &quot;C&quot; &quot;T&quot; &quot;GTTT&quot; &quot;GCCC&quot; &quot;C&quot; ## [14] &quot;C&quot; &quot;C&quot; &quot;CGAGCAT&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;CCATTCATT&quot; &quot;G&quot; # how many variants are biallelic SNVs? table(isSNV(gds, biallelic=TRUE)) ## ## FALSE TRUE ## 110 1010 # how many variants are SNVs vs INDELs? table(isSNV(gds, biallelic=FALSE)) ## ## FALSE TRUE ## 99 1021 # 11 SNVs are multi-allelic We can also return variant information as a GRanges object from the GenomicRanges package. This format for representing sequence data is common across many Bioconductor packages. Chromosome is stored in the seqnames column. The ranges column has variant position, which can be a single base pair or a range. gr &lt;- granges(gds) gr ## GRanges object with 1120 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## 1 1 970546 * ## 2 1 985900 * ## 3 1 1025045 * ## 4 1 1265550 * ## 5 1 1472676 * ## ... ... ... ... ## 1116 1 248715186 * ## 1117 1 248715606-248715610 * ## 1118 1 248761613 * ## 1119 1 248894546 * ## 1120 1 249149558 * ## ------- ## seqinfo: 1 sequence from an unspecified genome; no seqlengths Always use the seqClose command to close your connection to a GDS file when you are done working with it. Trying to open an already opened GDS will result in an error. seqClose(gds) 2.3 Exercises Set a filter selecting only multi-allelic variants. Inspect their genotypes using the different methods you learned above. Use the alleleDosage method to find dosage for the second (and third, etc.) alternate allele. Use the hwe function in SeqVarTools to run a Hardy-Weinberg Equilibrium test on each variant. Identify a variant with low p-value and inspect its genotypes. (Note that the HWE test is only valid for biallelic variants, and will return NA for multiallelic variants.) 2.4 Solutions Set a filter selecting only multi-allelic variants. Inspect their genotypes using the different methods you learned above. Use the alleleDosage method to find dosage for the second (and third, etc.) alternate allele. # open a connection to the GDS file again gds &lt;- seqOpen(gdsfile) # set your filter seqSetFilter(gds, variant.sel=multi.allelic) ## # of selected variants: 21 geno &lt;- seqGetData(gds, &quot;genotype&quot;) dim(geno) ## [1] 2 1126 21 geno[,1:5,1:5] ## , , 1 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 1 1 0 1 1 ## [2,] 0 1 1 1 1 ## ## , , 2 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## ## , , 3 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 ## ## , , 4 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 0 1 0 ## [2,] 0 0 0 0 1 ## ## , , 5 ## ## sample ## allele [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 0 0 0 0 0 geno &lt;- getGenotype(gds) dim(geno) ## [1] 1126 21 head(geno) ## variant ## sample 30 69 73 161 162 195 243 253 407 431 434 610 627 645 689 756 765 814 988 1014 1056 ## HG00096 &quot;1|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|1&quot; &quot;3|3&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|0&quot; &quot;2|2&quot; &quot;2|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00097 &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|2&quot; &quot;0|1&quot; &quot;1|3&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|2&quot; &quot;2|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00099 &quot;0|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;1|3&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|2&quot; &quot;2|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; ## HG00100 &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|2&quot; &quot;0|1&quot; &quot;1|1&quot; &quot;2|0&quot; &quot;0|0&quot; &quot;0|2&quot; &quot;2|2&quot; &quot;2|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|2&quot; ## HG00101 &quot;1|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|0&quot; &quot;1|1&quot; &quot;0|0&quot; &quot;3|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|0&quot; &quot;2|2&quot; &quot;2|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|0&quot; ## HG00102 &quot;0|1&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;1|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|2&quot; &quot;0|1&quot; &quot;3|3&quot; &quot;0|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;2|2&quot; &quot;1|2&quot; &quot;0|0&quot; &quot;0|0&quot; &quot;0|1&quot; &quot;0|2&quot; geno &lt;- getGenotypeAlleles(gds) head(geno) ## variant ## sample 30 69 73 161 162 195 243 253 407 431 434 610 627 645 689 756 765 814 988 ## HG00096 &quot;GT|GTT&quot; &quot;A|A&quot; &quot;G|G&quot; &quot;A|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;T|T&quot; &quot;TAA|TA&quot; &quot;G|GTTA&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;C|G&quot; &quot;CGAGCAT|CGAGCAT&quot; &quot;C|C&quot; &quot;CT|CT&quot; &quot;CGTGA|CGTGA&quot; ## HG00097 &quot;GT|GT&quot; &quot;A|A&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;T|T&quot; &quot;T|T&quot; &quot;G|GTTA&quot; &quot;GCC|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;CGAGCAT|CGAGCAT&quot; &quot;C|C&quot; &quot;CT|CT&quot; &quot;CGTGA|CGTGA&quot; ## HG00099 &quot;GTT|GT&quot; &quot;A|A&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;T|T&quot; &quot;TA|TA&quot; &quot;GTTA|G&quot; &quot;GCC|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;CGAGCAT|CGAGCAT&quot; &quot;C|C&quot; &quot;CT|CT&quot; &quot;CGTGA|CGTGA&quot; ## HG00100 &quot;GT|GT&quot; &quot;A|A&quot; &quot;G|G&quot; &quot;A|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;T|T&quot; &quot;TA|T&quot; &quot;G|GTTA&quot; &quot;GCC|GCC&quot; &quot;C|G&quot; &quot;G|G&quot; &quot;G|C&quot; &quot;CGAGCAT|CGAGCAT&quot; &quot;C|C&quot; &quot;CT|CT&quot; &quot;CGTGA|CGTGA&quot; ## HG00101 &quot;GT|GT&quot; &quot;A|A&quot; &quot;G|G&quot; &quot;G|A&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;C|T&quot; &quot;TAA|TAA&quot; &quot;G|G&quot; &quot;G|GC&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;C|G&quot; &quot;CGAGCAT|CGAGCAT&quot; &quot;C|C&quot; &quot;CT|CT&quot; &quot;CGTGA|CGTGA&quot; ## HG00102 &quot;GTT|GT&quot; &quot;A|A&quot; &quot;G|G&quot; &quot;A|G&quot; &quot;A|A&quot; &quot;A|A&quot; &quot;C|C&quot; &quot;G|G&quot; &quot;T|T&quot; &quot;T|T&quot; &quot;G|GTTA&quot; &quot;G|G&quot; &quot;G|C&quot; &quot;G|G&quot; &quot;G|G&quot; &quot;CGAGCAT|CGAGCAT&quot; &quot;CATTATT|C&quot; &quot;CT|CT&quot; &quot;CGTGA|CGTGA&quot; ## variant ## sample 1014 1056 ## HG00096 &quot;C|C&quot; &quot;A|A&quot; ## HG00097 &quot;C|C&quot; &quot;A|A&quot; ## HG00099 &quot;C|C&quot; &quot;A|A&quot; ## HG00100 &quot;C|C&quot; &quot;A|G&quot; ## HG00101 &quot;CCATT|C&quot; &quot;A|A&quot; ## HG00102 &quot;C|CCATT&quot; &quot;A|G&quot; # count of reference alleles dos &lt;- refDosage(gds) head(dos) ## variant ## sample 30 69 73 161 162 195 243 253 407 431 434 610 627 645 689 756 765 814 988 1014 1056 ## HG00096 1 2 2 1 2 2 2 2 2 1 1 0 2 2 1 0 0 2 2 2 2 ## HG00097 0 2 2 2 2 2 2 2 2 0 1 0 2 2 2 0 0 2 2 2 2 ## HG00099 1 2 2 2 2 2 2 2 2 2 1 0 2 2 2 0 0 2 2 2 2 ## HG00100 0 2 2 1 2 2 2 2 2 1 1 0 1 2 1 0 0 2 2 2 1 ## HG00101 0 2 2 1 2 2 2 2 1 0 2 1 2 2 1 0 0 2 2 1 2 ## HG00102 1 2 2 1 2 2 2 2 2 0 1 0 1 2 2 0 0 2 2 1 1 # count of *any* alternate alleles dos &lt;- altDosage(gds) head(dos) ## variant ## sample 30 69 73 161 162 195 243 253 407 431 434 610 627 645 689 756 765 814 988 1014 1056 ## HG00096 1 0 0 1 0 0 0 0 0 1 1 2 0 0 1 2 2 0 0 0 0 ## HG00097 2 0 0 0 0 0 0 0 0 2 1 2 0 0 0 2 2 0 0 0 0 ## HG00099 1 0 0 0 0 0 0 0 0 0 1 2 0 0 0 2 2 0 0 0 0 ## HG00100 2 0 0 1 0 0 0 0 0 1 1 2 1 0 1 2 2 0 0 0 1 ## HG00101 2 0 0 1 0 0 0 0 1 2 0 1 0 0 1 2 2 0 0 1 0 ## HG00102 1 0 0 1 0 0 0 0 0 2 1 2 1 0 0 2 2 0 0 1 1 # count of the first alternate allele dos &lt;- alleleDosage(gds, n=1) head(dos) ## variant ## sample 30 69 73 161 162 195 243 253 407 431 434 610 627 645 689 756 765 814 988 1014 1056 ## HG00096 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 ## HG00097 2 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 ## HG00099 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 ## HG00100 2 0 0 1 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 0 0 ## HG00101 2 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 0 ## HG00102 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 # count of the third alternate allele dos &lt;- alleleDosage(gds, n=3) head(dos) ## variant ## sample 30 69 73 161 162 195 243 253 407 431 434 610 627 645 689 756 765 814 988 1014 1056 ## HG00096 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 ## HG00097 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## HG00099 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## HG00100 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## HG00101 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 ## HG00102 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 # count of *each* of the alternate alleles # returns multiple columns per variant dos &lt;- expandedAltDosage(gds) head(dos) ## variant ## sample 30 30 69 69 73 73 161 161 162 162 195 195 243 243 253 253 407 407 431 431 434 434 610 610 610 627 627 645 645 689 689 756 756 765 765 814 814 988 988 1014 ## HG00096 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 2 0 0 0 0 0 1 0 2 0 2 0 0 0 0 0 ## HG00097 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 1 0 1 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 ## HG00099 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 2 0 2 0 0 0 0 0 ## HG00100 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 2 0 0 0 1 0 0 0 1 0 2 0 2 0 0 0 0 0 ## HG00101 2 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 1 0 0 0 0 0 1 0 2 0 2 0 0 0 0 1 ## HG00102 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 2 0 1 0 0 0 0 0 2 1 1 0 0 0 0 1 ## variant ## sample 1014 1056 1056 ## HG00096 0 0 0 ## HG00097 0 0 0 ## HG00099 0 0 0 ## HG00100 0 0 1 ## HG00101 0 0 0 ## HG00102 0 0 1 Use the hwe function in SeqVarTools to run a Hardy-Weinberg Equilibrium test on each variant. Identify a variant with low p-value and inspect its genotypes. (Note that the HWE test is only valid for biallelic variants, and will return NA for multiallelic variants.) # reset the filter to all variants seqResetFilter(gds) ## # of selected samples: 1,126 ## # of selected variants: 1,120 # run HWE test hwe.res &lt;- hwe(gds) # identify variants with small p-values lowp &lt;- !is.na(hwe.res$p) &amp; hwe.res$p &lt; 1e-4 head(hwe.res[lowp,]) ## variant.id nAA nAa naa afreq p f ## 75 75 702 336 88 0.7726465 1.070663e-06 0.1506466 ## 92 92 632 381 113 0.7304618 3.558878e-06 0.1407120 ## 98 98 672 335 119 0.7455595 2.369695e-12 0.2158342 ## 105 105 93 272 761 0.2033748 7.851777e-16 0.2544970 ## 114 114 299 482 345 0.4795737 1.745346e-06 0.1424409 ## 150 150 471 447 208 0.6167851 8.020208e-08 0.1602251 # look at the genotypes of the most significant variant minp &lt;- which.min(hwe.res$p) hwe.res[minp,] ## variant.id nAA nAa naa afreq p f ## 608 608 76 1048 2 0.5328597 5.211823e-234 -0.8695311 seqSetFilter(gds, variant.id=minp) ## # of selected variants: 1 table(getGenotype(gds)) ## ## 0|0 0|1 1|0 1|1 ## 76 559 489 2 table(altDosage(gds)) ## ## 0 1 2 ## 76 1048 2 Don’t forget to close your connection to the GDS file when you’re done! seqClose(gds) "],["phenotype-harmonization.html", "3 Phenotype Harmonization 3.1 Inspect individual study data in R 3.2 Compare study values 3.3 Using regression models to compare studies 3.4 Final considerations", " 3 Phenotype Harmonization To increase your sample set, you may need to combine phenotype data from different studies in order to run a cross-study analysis. The studies involved may have collected data in different ways, used different protocols or measurement units, or used different cutpoints to determine case status. The process of manipulating the phenotype data from different studies so that they can be analyzed together is called “phenotype harmonization”. In this exercise, we assume that you have created a phenotype harmonization plan for height, sent it to members from three studies to perform the harmonization, and received a harmonized phenotype file from each study. We will generate some diagnostic information about the harmonized phenotype. The exercise uses 1000 Genomes data, with simulated phenotypes for study, age, and height. The example phenotype files shown here are very simplified compared to how actual studies store and organize their their data. In this exercise, we will be using dplyr for a lot of the data manipulation, so load it now. library(dplyr) 3.1 Inspect individual study data in R The first step is to read the files into R for processing. repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) pheno_files &lt;- c(&quot;data/pheno_data_study_1.txt&quot;, &quot;data/pheno_data_study_2.txt&quot;, &quot;data/pheno_data_study_3.txt&quot;) for (pheno_file in pheno_files) { if (!file.exists(pheno_file)) download.file(file.path(repo_path, pheno_file), pheno_file) } Next, read the study phenotype files into R. In this case, each file is tab-delimited. study_1 &lt;- read.table(&quot;data/pheno_data_study_1.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, as.is = TRUE) head(study_1) ## subject_id sex age height ## 1 HG00096 M 47 165.3 ## 2 HG00102 F 49 169.1 ## 3 HG00112 M 46 167.9 ## 4 HG00114 M 49 169.5 ## 5 HG00115 M 35 161.1 ## 6 HG00116 M 37 182.2 study_2 &lt;- read.table(&quot;data/pheno_data_study_2.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, as.is = TRUE) head(study_2) ## subject_id Sex Age Height ## 1 HG00099 F 40 185.5 ## 2 HG00103 M 50 190.8 ## 3 HG00106 F 51 165.5 ## 4 HG00107 M 39 195.8 ## 5 HG00109 M 48 181.5 ## 6 HG00111 F 42 194.9 study_3 &lt;- read.table(&quot;data/pheno_data_study_3.txt&quot;, header = TRUE, sep = &quot;\\t&quot;, as.is = TRUE) head(study_3) ## subject_id sex age height ## 1 HG00097 F 47 57.0 ## 2 HG00100 F 45 59.3 ## 3 HG00101 M 40 70.0 ## 4 HG00105 M 34 62.4 ## 5 HG00108 M 47 66.3 ## 6 HG00110 F 44 62.7 Look carefully at the output and see if anything looks suspicious. You may have noticed that one of the studies has given their variables slightly different names than the others. Rename them as appropriate. names(study_2) ## [1] &quot;subject_id&quot; &quot;Sex&quot; &quot;Age&quot; &quot;Height&quot; study_2 &lt;- study_2 %&gt;% rename(sex = Sex, age = Age, height = Height) # Check that they are correct. names(study_2) ## [1] &quot;subject_id&quot; &quot;sex&quot; &quot;age&quot; &quot;height&quot; You’ll also want to calculate summaries of the data values to see if anything looks very different than what you expect. summary(study_1$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 139.6 163.8 169.9 170.2 176.7 200.3 summary(study_2$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 142.1 171.8 181.3 180.8 190.5 218.6 summary(study_3$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 47.00 59.60 63.40 63.42 67.10 79.60 Here, the values that study_3 has given you don’t seem to have the same range as those from study_1 and study_2. In cases like this, you’ll want to follow up with whoever provided the harmonized data to see what’s going on. It could represent an error in calculating the harmonized data values, a true property of the study (e.g., a study containing all children), or something else. In this case, the values were measured in inches instead of centimeters, so they will need to be converted to centimeters to be compatible with the other studies. study_3 &lt;- study_3 %&gt;% mutate(height = height * 2.54) Calculate the summary again and compare it to the other studies above. summary(study_3$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 119.4 151.4 161.0 161.1 170.4 202.2 The corrected values look much more similar now. Note that this sort of error is easy to correct, but it is not uncommon to have more subtle issues that need to be addressed when working with phenotype data. Knowledge of the study design as well as the phenotype area of interest is essential to address them properly. Additionally, different decisions may need to be made for different analyses based on the specific questions they are trying to answer. 3.2 Compare study values Next we will make some more direct comparisons between the three studies, so we will combine the data into one data frame. First, add a study identifier to the data frame for organizational purposes. study_1$study &lt;- &quot;study_1&quot; study_2$study &lt;- &quot;study_2&quot; study_3$study &lt;- &quot;study_3&quot; Combine the three different study data frames into one large data frame for joint analysis. Double check that all column names are the same. all.equal(names(study_1), names(study_2)) ## [1] TRUE all.equal(names(study_1), names(study_3)) ## [1] TRUE phen &lt;- dplyr::bind_rows(study_1, study_2, study_3) We can look at the distribution of phenotype data with text-based reports or with plots. First, inspect distributions with table for categorical traits and with summary for quantitative traits. The commads are shown here for study_1, but you should run them for study_2 and study_3 as well to see if you can see any differences. table(study_1$sex) ## ## F M ## 190 185 summary(study_1$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 32.00 41.00 45.00 45.17 49.00 62.00 summary(study_1$height) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 139.6 163.8 169.9 170.2 176.7 200.3 It is also helpful to use plots to inspect the distributions of phenotype data. Here, we will look at boxplots of height by study. library(ggplot2) ggplot(phen, aes(x = study, y = height)) + geom_boxplot() You may also want to see the difference in height when you include both study and sex: ggplot(phen, aes(x = study, fill = sex, y = height)) + geom_boxplot() These diagnostics are helpful to get a feel for the data. They can help you see if one study is vastly different from the others or detect outlier values that you may want to look into further. Some of the differences could also be accounted for by covariates. 3.3 Using regression models to compare studies The quick diagnostics in the previous section let you see if the data from one study are completely different from the others, but such differences could be due to other factors that could be adjusted for in analysis. To account for these other factors, we need to fit a statistical model to the data. In this case, because the phenotype is quantitative, we will use a linear regression model. We use the GENESIS R package for fitting the regression model. It is also the same package that we use for the association analyses, so this exercise provides a brief introduction to the package and some of the associated data structures. 3.3.1 Create an Annotated Data Frame The first step in fitting the regression model is to create an AnnotatedDataFrame. This data structure is provided by the Bioconductor Biobase package, and it contains both the data and metadata. You should include a description of each variable in the metadata. library(Biobase) metadata &lt;- data.frame(labelDescription = c( &quot;subject identifier&quot;, &quot;subject&#39;s sex&quot;, &quot;age at measurement of height&quot;, &quot;subject&#39;s height in cm&quot;, &quot;study identifier&quot; )) annot &lt;- AnnotatedDataFrame(phen, metadata) # Access the data with the pData() function. head(pData(annot)) ## subject_id sex age height study ## 1 HG00096 M 47 165.3 study_1 ## 2 HG00102 F 49 169.1 study_1 ## 3 HG00112 M 46 167.9 study_1 ## 4 HG00114 M 49 169.5 study_1 ## 5 HG00115 M 35 161.1 study_1 ## 6 HG00116 M 37 182.2 study_1 # Access the metadata with the varMetadata() function. varMetadata(annot) ## labelDescription ## subject_id subject identifier ## sex subject&#39;s sex ## age age at measurement of height ## height subject&#39;s height in cm ## study study identifier Save the AnnotatedDataFrame for future use. save(annot, file = &quot;data/phenotype_annotation.RData&quot;) The GENESIS code to fit the regression model also requires a sample.id column. Typically the sample.id column represents a sample identifier, not a subject id. In this case, we are only working with subject-level data, so we can use the subject identifier as the sample identifier for model-fitting purposes. annot$sample.id &lt;- annot$subject_id 3.3.2 Fit a regression model without study We will first fit a regression model that allows us to see if the mean of the height phenotype is different by study after adjusting for other covariates. In this case, we will adjust for age and sex, but not for study, because we are interested in seeing differences in mean height by study. We use the fitNullModel function from the GENESIS package – the name “null model” comes from association testing context and will be explained later. outcome &lt;- &quot;height&quot; covars &lt;- c(&quot;sex&quot;, &quot;age&quot;) library(GENESIS) mod_1 &lt;- GENESIS::fitNullModel(annot, outcome = outcome, covars = covars) The output of fitNullModel is a list with a number of named elements names(mod_1) ## [1] &quot;model&quot; &quot;varComp&quot; &quot;varCompCov&quot; &quot;fixef&quot; &quot;betaCov&quot; &quot;fit&quot; &quot;logLik&quot; &quot;AIC&quot; &quot;model.matrix&quot; &quot;group.idx&quot; ## [11] &quot;cholSigmaInv&quot; &quot;converged&quot; &quot;zeroFLAG&quot; &quot;RSS&quot; &quot;CX&quot; &quot;CXCXI&quot; &quot;RSS0&quot; The elements that we will work with in this exercise are: converged: an indicator of whether the model successfully converged model.matrix: The matrix of subject-covariate values used to fit the model fixef: The fitted fixed effects betaCov: The covariance of the fitted fixed effects fit: A data frame containing information about the fit, in particular: resid.marginal: The (marginal) residuals from the model, which have been adjusted for the fixed effects but not for the covariance structure varComp: The fitted variance components Make sure the model converged. mod_1$converged ## [1] TRUE Now, add the residuals to the phenotype data frame for plotting. We need to make sure that we are matching each residual value to the correct subject. In this case, model.matrix is already in the same order as the input AnnotatedDataFrame, but this may not always be the case (for example, if subjects are excluded due to missing phentoype data). To match the same subject’s values together, we use the rownames of the fit data frame to match to the subject_id column of the annotated data frame. We then match the row names (and therefore the residuals) to the sample identifier in the phenotype file using the base R function match. j &lt;- match(annot$sample.id, rownames(mod_1$fit)) annot$residuals &lt;- mod_1$fit$resid.marginal[j] Next, we want to check if the different studies have the same mean height after adjustment for other covariates (here, age and sex). We will first do this qualitatively by making a boxplot of the residuals by study. ggplot(pData(annot), aes(x = study, y = residuals)) + geom_boxplot() From the boxplot, it is clear that the different studies have different mean heights, even after adjustment for sex and age. At this point, you would need to determine if the differences are acceptable for use in a combined analysis. 3.3.3 Fit a model with study Next, we can look at a model that adjusts for other covariates as well as study. This model allows us to run a statistical test on the fitted study means and to qualitatively check if the variances are the same after adjusting for mean effects. The outcome is the same, but we now add the study as a covariate. We also allow for group-specific residual variance by study using the group.var argument to fitNullModel. # include the study in the covariates covars &lt;- c(&quot;age&quot;, &quot;sex&quot;, &quot;study&quot;) mod_2 &lt;- GENESIS::fitNullModel(annot, outcome = outcome, covars = covars, group.var = &quot;study&quot;) The fixef element now includes effects for study: mod_2$fixef ## Est SE Stat pval ## (Intercept) 163.67175933 3.18936046 2633.542247 0.000000e+00 ## age 0.07519782 0.06921691 1.180283 2.772984e-01 ## sexM 6.28764509 0.68812251 83.491932 6.397653e-20 ## studystudy_2 10.63152325 0.82176939 167.375182 2.769992e-38 ## studystudy_3 -8.96183691 0.84479021 112.537257 2.724959e-26 The regression model also shows the differences in mean height by study. Finally, we want to check if the height distributions from the different studies have the same variance. Start by looking at the variance components (varComp) element of the model. mod_2$varComp ## V_study_1 V_study_2 V_study_3 ## 98.20191 155.70722 168.82044 The variance components (V_study_1, V_study_2, and V_study_3) represent the residual variance in each study. The fitted values of the variance components are different for the different studies, indicating that the distributions of height in the three studies have different variance even after accounting for the other covariates. We can also show the same information by plotting the residuals by study. We first have to add the residuals from this model to the AnnotatedDataFrame. annot$residuals &lt;- mod_2$fit$resid.marginal[match(annot$sample.id, rownames(mod_2$fit))] Next make a boxplot of the residuals by study. ggplot(pData(annot), aes(x = study, y = residuals)) + geom_boxplot() Both methods of looking at the variance components indicate that study 1 has a smaller residual variance than the others. 3.4 Final considerations We have determined that the different studies have both different mean and different variance by study for height. Before performing genotype-phenotype association tests with these data, you would need to think carefully about whether the phenotype is homogeneous enough to be analyzed together. In some cases, there may be a valid reason for different means or variances, for example: different heights in different study populations, such as a study composed primarily of Asian participants vs. a study with primarily European participants or a study of all men vs. a study of all women; possible secular trends in height, such as comparing the Framingham Original cohort from ~1950 to a cohort from the present day. In other cases, there may be good reasons to exclude one or more studies, for example: a systematic measurement error in one study miscalculation or misinterpretation of the harmonization algorithm study populations that are too different to be compared, such as trying to include a study composed primarily of children with one composed of adults in a height analysis It may be necessary to look at other variables that you had not previously considered. Studies may have used different measurement equipment or calibrated their data differently. There might also be other batch effects due to lab procedures or assays that could result in differences in the variance or mean by study. The other variables that you may need to consider are highly dependent both on the phenotype being harmonized and on how a given study has been designed. Unfortunately there is no single set of guidelines you can use to decide how to proceed with analysis of a phenotype. It is necessary to involve both domain experts and study experts to determine whether the phenotype is homogeneous enough to use in cross-study analysis. "],["association-tests-part-i.html", "4 Association tests - Part I 4.1 Null model 4.2 Exercise 4.3 Solution 4.4 Single-variant association tests 4.5 Exercise 4.6 Solution", " 4 Association tests - Part I These exercises introduce genetic association testing: how to identify which genetic variants are associated with a phenotype. In this example, we will test for an association between variant genotypes and height, adjusting for sex, age, and study. Here, we introduce fitting the “null model” and single-variant association testing, as is commonly performed in GWAS (Genome Wide Association Studies). 4.1 Null model The first step in our association testing procedure is to fit the null model – i.e., a model fit under the null hypothesis of no individual variant association. Operationally, this is fitting a regression model with the desired outcome phenotype, fixed effect covariates, and random effects. 4.1.1 Prepare the data To fit the null model, we will need to create an AnnotatedDataFrame with sample information and phenotype data; this class is defined in the Biobase package. We will merge our sample annotation file, which is indexed by a sample.id column matched to the GDS file, with our phenotype file, which is indexed by a subject_id column. We will use the dplyr package for data.frame manipulation. NOTE: In this example, we use the 1000 Genomes IDs for both sample and subject IDs, though we would generally advise using separate IDs for samples (sequencing instances) and subjects (individuals). # sample annotation repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) sampfile &lt;- &quot;data/sample_annotation.RData&quot; if (!file.exists(sampfile)) download.file(file.path(repo_path, sampfile), sampfile) samp &lt;- get(load(sampfile)) library(Biobase) # access the data with the pData() function head(pData(samp)) ## sample.id subject.id Population Population.Description sex status ## 1 HG00096 HG00096 GBR British in England and Scotland M 0 ## 2 HG00097 HG00097 GBR British in England and Scotland F 1 ## 3 HG00099 HG00099 GBR British in England and Scotland F 0 ## 4 HG00100 HG00100 GBR British in England and Scotland F 1 ## 5 HG00101 HG00101 GBR British in England and Scotland M 0 ## 6 HG00102 HG00102 GBR British in England and Scotland F 0 # access the metadata with the varMetadata() function varMetadata(samp) ## labelDescription ## sample.id sample identifier ## subject.id subject identifier ## Population population abbreviation ## Population.Description population description ## sex sex ## status simulated case/control status # phenotype data phenfile &lt;- &quot;data/phenotype_annotation.RData&quot; if (!file.exists(phenfile)) download.file(file.path(repo_path, phenfile), phenfile) phen &lt;- get(load(phenfile)) # access the data with the pData() function head(pData(phen)) ## subject_id sex age height study ## 1 HG00096 M 47 165.3 study_1 ## 2 HG00102 F 49 169.1 study_1 ## 3 HG00112 M 46 167.9 study_1 ## 4 HG00114 M 49 169.5 study_1 ## 5 HG00115 M 35 161.1 study_1 ## 6 HG00116 M 37 182.2 study_1 # access the metadata with the varMetadata() function varMetadata(phen) ## labelDescription ## subject_id subject identifier ## sex subject&#39;s sex ## age age at measurement of height ## height subject&#39;s height in cm ## study study identifier # merge sample annotation with phenotype data library(dplyr) dat &lt;- pData(samp) %&gt;% left_join(pData(phen), by=c(&quot;subject.id&quot;=&quot;subject_id&quot;, &quot;sex&quot;=&quot;sex&quot;)) head(dat) ## sample.id subject.id Population Population.Description sex status age height study ## 1 HG00096 HG00096 GBR British in England and Scotland M 0 47 165.300 study_1 ## 2 HG00097 HG00097 GBR British in England and Scotland F 1 47 144.780 study_3 ## 3 HG00099 HG00099 GBR British in England and Scotland F 0 40 185.500 study_2 ## 4 HG00100 HG00100 GBR British in England and Scotland F 1 45 150.622 study_3 ## 5 HG00101 HG00101 GBR British in England and Scotland M 0 40 177.800 study_3 ## 6 HG00102 HG00102 GBR British in England and Scotland F 0 49 169.100 study_1 # merge the metadata meta &lt;- bind_rows(varMetadata(samp), varMetadata(phen)[3:5,,drop=FALSE]) # make an AnnotatedDataFrame annot &lt;- AnnotatedDataFrame(dat, meta) save(annot, file=&quot;data/sample_phenotype_annotation.RData&quot;) 4.1.2 Fit the null model We use the fitNullModel function from the GENESIS package to fit the null model. We need to specify the outcome (height) and the fixed effect covariates (sex, age, and study). If the sample set involves multiple distinct groups with different variances for the phenotype, we recommend allowing for heterogeneous residual variance among groups with the group.var parameter. We saw in a previous exercise that the variance of height differs by study. library(GENESIS) # fit the null model nullmod &lt;- fitNullModel(annot, outcome=&quot;height&quot;, covars=c(&quot;sex&quot;, &quot;age&quot;, &quot;study&quot;), group.var=&quot;study&quot;, verbose=FALSE) save(nullmod, file=&quot;data/null_model.RData&quot;) The fitNullModel function returns a lot of information about the model that was fit. We examine some of that information below; to see all of the components, try names(nullmod). # description of the model we fit nullmod$model ## $hetResid ## [1] TRUE ## ## $family ## ## Family: gaussian ## Link function: identity ## ## ## $outcome ## [1] &quot;height&quot; ## ## $covars ## [1] &quot;sex&quot; &quot;age&quot; &quot;study&quot; ## ## $formula ## [1] &quot;height ~ sex + age + study + var(study)&quot; # fixed effect regression estimates nullmod$fixef ## Est SE Stat pval ## (Intercept) 163.67175933 3.18936046 2633.542247 0.000000e+00 ## sexM 6.28764509 0.68812251 83.491932 6.397653e-20 ## age 0.07519782 0.06921691 1.180283 2.772984e-01 ## studystudy_2 10.63152325 0.82176939 167.375182 2.769992e-38 ## studystudy_3 -8.96183691 0.84479021 112.537257 2.724959e-26 # residual variance estimates by group.var nullmod$varComp ## V_study_1 V_study_3 V_study_2 ## 98.20191 168.82044 155.70722 # model fit: fitted values, residuals head(nullmod$fit) ## outcome workingY fitted.values resid.marginal resid.PY resid.cholesky sample.id ## HG00096 165.300 165.300 173.4937 -8.193702 -0.08343729 -0.8268376 HG00096 ## HG00097 144.780 144.780 158.2442 -13.464220 -0.07975468 -1.0362599 HG00097 ## HG00099 185.500 185.500 177.3112 8.188805 0.05259104 0.6562452 HG00099 ## HG00100 150.622 150.622 158.0938 -7.471824 -0.04425900 -0.5750613 HG00100 ## HG00101 177.800 177.800 164.0055 13.794520 0.08171119 1.0616810 HG00101 ## HG00102 169.100 169.100 167.3565 1.743547 0.01775472 0.1759437 HG00102 # plot the residuals vs the fitted values library(ggplot2) ggplot(nullmod$fit, aes(x = fitted.values, y = resid.marginal)) + geom_point(alpha = 0.5) + geom_hline(yintercept = 0) + geom_smooth(method = &#39;lm&#39;) 4.2 Exercise As discussed in the lecture, we recommend a fully adjusted two-stage inverse Normalization procedure for fitting the null model when phenotypes have non-Normal distributions. Using the two.stage option in fitNullModel, fit a two-stage null model. Compare these residuals with the residuals from the original null model. 4.3 Solution As discussed in the lecture, we recommend a fully adjusted two-stage inverse Normalization procedure for fitting the null model when phenotypes have non-Normal distributions. Using the two.stage option in fitNullModel, fit a two-stage null model. Compare these residuals with the residuals from the original null model. To run the fully adjusted two.stage null model, we simply set the two.stage option to TRUE. The norm.option parameter determines if the inverse Normalization should be done with all samples together (\"all\") or within each group.var group separately (\"by.group\"). nullmod.twostage &lt;- fitNullModel(annot, outcome=&quot;height&quot;, covars=c(&quot;sex&quot;, &quot;age&quot;, &quot;study&quot;), group.var=&quot;study&quot;, two.stage = TRUE, norm.option = &quot;all&quot;, verbose=FALSE) save(nullmod.twostage, file=&quot;data/null_model_two_stage.RData&quot;) # description of the model we fit nullmod.twostage$model ## $hetResid ## [1] TRUE ## ## $family ## ## Family: gaussian ## Link function: identity ## ## ## $outcome ## [1] &quot;height&quot; ## ## $covars ## [1] &quot;sex&quot; &quot;age&quot; &quot;study&quot; ## ## $formula ## [1] &quot;rankInvNorm(resid(height)) ~ sex + age + study + var(study)&quot; # compare the marginal residuals # merge the data for plotting pdat &lt;- merge(nullmod$fit, nullmod.twostage$fit, by = &#39;sample.id&#39;, suffixes = c(&#39;.orig&#39;, &#39;.twostage&#39;)) pdat &lt;- merge(pdat, pData(annot), by = &#39;sample.id&#39;) # distribution of residuals - original null model ggplot(pdat, aes(x = resid.marginal.orig)) + geom_density(aes(color = study)) + geom_density(size = 2) # distribution of residuals - two stage null model ggplot(pdat, aes(x = resid.marginal.twostage)) + geom_density(aes(color = study)) + geom_density(size = 2) # compare residuals ggplot(pdat, aes(x = resid.marginal.orig, y = resid.marginal.twostage, color = study)) + geom_point() + geom_abline(intercept = 0, slope = 1) There is not much difference in the residual here because the distribution of height is not far from Normal to begin. See Sofer et al. for more information on the fully adjusted two-stage model. 4.4 Single-variant association tests After fitting our null model, we can use score tests to test each variant across the genome individually for association with the outcome phenotype (i.e. height in our example). Performing these single-variant tests genome-wide is commonly referred to as a GWAS (Genome-Wide Association Study). We use the assocTestSingle function in GENESIS. First, we have to create a SeqVarData object including both the GDS file and the sample annotation containing phenotype data. We then create a SeqVarBlockIterator object, which breaks the set of all variants in the SeqVarData object into blocks, allowing us to analyze genome-wide in manageable pieces. The assocTestSingle function iterates over all blocks of variants in the SeqVarBlockIterator object and then concatenates and returns the results. library(SeqVarTools) gdsfile &lt;- &quot;data/1KG_phase3_subset_chr1.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(repo_path, gdsfile), gdsfile) gdsfmt::showfile.gds(closeall=TRUE) # make sure file is not already open gds &lt;- seqOpen(gdsfile) # make the seqVarData object seqData &lt;- SeqVarData(gds, sampleData=annot) # make the iterator object iterator &lt;- SeqVarBlockIterator(seqData, verbose=FALSE) iterator ## SeqVarBlockIterator object; on iteration 1 of 1 ## | GDS: ## File: /Users/mconomos/Documents/SISG_2021/data/1KG_phase3_subset_chr1.gds (70.5K) ## + [ ] * ## |--+ description [ ] * ## |--+ sample.id { Str8 1126 LZMA_ra(9.66%), 877B } * ## |--+ variant.id { Int32 1120 LZMA_ra(17.5%), 793B } * ## |--+ position { Int32 1120 LZMA_ra(78.5%), 3.4K } * ## |--+ chromosome { Str8 1120 LZMA_ra(4.55%), 109B } * ## |--+ allele { Str8 1120 LZMA_ra(26.0%), 1.2K } * ## |--+ genotype [ ] * ## | |--+ data { Bit2 2x1126x1121 LZMA_ra(8.34%), 51.4K } * ## | |--+ extra.index { Int32 3x0 LZMA_ra, 18B } * ## | \\--+ extra { Int16 0 LZMA_ra, 18B } ## |--+ phase [ ] ## | |--+ data { Bit1 1126x1120 LZMA_ra(0.11%), 177B } * ## | |--+ extra.index { Int32 3x0 LZMA_ra, 18B } * ## | \\--+ extra { Bit1 0 LZMA_ra, 18B } ## |--+ annotation [ ] ## | |--+ id { Str8 1120 LZMA_ra(40.4%), 3.6K } * ## | |--+ qual { Float32 1120 LZMA_ra(2.46%), 117B } * ## | |--+ filter { Int32,factor 1120 LZMA_ra(2.46%), 117B } * ## | |--+ info [ ] ## | \\--+ format [ ] ## \\--+ sample.annotation [ ] ## | sampleData: ## An object of class &#39;AnnotatedDataFrame&#39; ## rowNames: 1 2 ... 1126 (1126 total) ## varLabels: sample.id subject.id ... study (9 total) ## varMetadata: labelDescription ## | variantData: ## An object of class &#39;AnnotatedDataFrame&#39;: none # run the single-variant association test assoc &lt;- assocTestSingle(iterator, nullmod) ## # of selected samples: 1,126 dim(assoc) ## [1] 1129 14 head(assoc) ## variant.id chr pos allele.index n.obs freq MAC Score Score.SE Score.Stat Score.pval Est Est.SE PVE ## 1 1 1 970546 1 1126 0.0039964476 9 -0.1191236 0.2577712 -0.4621292 0.643988693 -1.792788 3.879410 0.0001905115 ## 2 2 1 985900 1 1126 0.0492895204 111 -1.6707553 0.8841849 -1.8895996 0.058811535 -2.137109 1.130985 0.0031851797 ## 3 3 1 1025045 1 1126 0.0004440497 1 -0.2795838 0.1007173 -2.7759261 0.005504472 -27.561563 9.928781 0.0068740102 ## 4 4 1 1265550 1 1126 0.0008880995 2 -0.1105487 0.1085480 -1.0184319 0.308472744 -9.382319 9.212515 0.0009252485 ## 5 5 1 1472676 1 1126 0.0071047957 16 0.3630992 0.3456555 1.0504657 0.293504065 3.039054 2.893054 0.0009843694 ## 6 6 1 1735725 1 1126 0.0022202487 5 -0.1300405 0.1973175 -0.6590420 0.509868790 -3.340007 5.067973 0.0003874544 We make a QQ plot to examine the results. library(ggplot2) qqPlot &lt;- function(pval) { pval &lt;- pval[!is.na(pval)] n &lt;- length(pval) x &lt;- 1:n dat &lt;- data.frame(obs=sort(pval), exp=x/n, upper=qbeta(0.025, x, rev(x)), lower=qbeta(0.975, x, rev(x))) ggplot(dat, aes(-log10(exp), -log10(obs))) + geom_line(aes(-log10(exp), -log10(upper)), color=&quot;gray&quot;) + geom_line(aes(-log10(exp), -log10(lower)), color=&quot;gray&quot;) + geom_point() + geom_abline(intercept=0, slope=1, color=&quot;red&quot;) + xlab(expression(paste(-log[10], &quot;(expected P)&quot;))) + ylab(expression(paste(-log[10], &quot;(observed P)&quot;))) + theme_bw() } qqPlot(assoc$Score.pval) A lot of the variants we tested are very rare – the alternate allele is not observed for many samples. Single-variant tests do not perform well for very rare variants (we will discuss testing rare variants in more detail in the next session). We can use the minor allele count (MAC) observed in the sample to filter rare variants that we may expect to have unreliable test results. summary(assoc$MAC) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 1.00 3.00 86.62 30.00 1123.00 sum(assoc$MAC &lt; 5) ## [1] 619 qqPlot(assoc$Score.pval[assoc$MAC &gt;= 5]) We should expect the majority of variants to fall near the red y=x line in the QQ plot. The deviation above the line, commonly referred to as “inflation” is indicative of some model issue. In this example, the issue is likely driven by the fact that we’ve ignored genetic ancestry and relatedness among these subjects – more to come later when we discuss mixed models. 4.5 Exercise GENESIS also supports testing binary (e.g. case/control) outcomes. We can fit a null model using logistic regression by specifying the argument family=binomial in the fitNullModel function. Use the status column in the sample annotation to fit a null model for simulated case/control status, with sex and Population as covariates. Run single-variant association tests using this model and make a QQ plot of all variants with MAC &gt;= 5. 4.6 Solution GENESIS also supports testing binary (e.g. case/control) outcomes. We can fit a null model using logistic regression by specifying the argument family=binomial in the fitNullModel function. Use the status column in the sample annotation to fit a null model for simulated case/control status, with sex and Population as covariates. Run single-variant association tests using this model and make a QQ plot of all variants with MAC &gt;= 5. When testing binary outcomes, we should fit our null model using logistic regression. To do so, we simply set the argument family=binomial in fitNullModel. Note that the parameter group.var is no longer relevant here, as the logistic model specifies the mean-variance relationship. # fit the null model with logistic regression nullmod.status &lt;- fitNullModel(annot, outcome=&quot;status&quot;, covars=c(&quot;sex&quot;, &quot;Population&quot;), family=binomial, verbose=FALSE) resetIterator(iterator, verbose=FALSE) # run the single-variant association test assoc.status &lt;- assocTestSingle(iterator, nullmod.status, test=&quot;Score&quot;) ## # of selected samples: 1,126 dim(assoc.status) ## [1] 1129 14 head(assoc.status) ## variant.id chr pos allele.index n.obs freq MAC Score Score.SE Score.Stat Score.pval Est Est.SE PVE ## 1 1 1 970546 1 1126 0.0039964476 9 0.20256722 0.8351783 0.2425437 0.80835892 0.2904095 1.1973491 5.242118e-05 ## 2 2 1 985900 1 1126 0.0492895204 111 -2.64169956 2.6522412 -0.9960254 0.31923781 -0.3755410 0.3770396 8.840314e-04 ## 3 3 1 1025045 1 1126 0.0004440497 1 -0.09916904 0.2972472 -0.3336248 0.73866267 -1.1223819 3.3642035 9.918446e-05 ## 4 4 1 1265550 1 1126 0.0008880995 2 0.81717324 0.4033577 2.0259271 0.04277226 5.0226565 2.4791892 3.657417e-03 ## 5 5 1 1472676 1 1126 0.0071047957 16 0.64418361 1.0778277 0.5976685 0.55006117 0.5545121 0.9277921 3.183080e-04 ## 6 6 1 1735725 1 1126 0.0022202487 5 -0.46319177 0.6396675 -0.7241134 0.46899613 -1.1320153 1.5633123 4.672400e-04 # make a QQ plot qqPlot(assoc.status$Score.pval[assoc.status$MAC &gt;= 5]) Extra: in samples with highly imbalanced case:control ratios, the Score test can perform poorly for low frequency variants. Saddlepoint approximation (SPA) can be used to improve p-value calculations, and is available in GENESIS by setting the argument test=Score.SPA in assocTestSingle. See Dey et al. and Zhou et al. for details on using SPA in GWAS. # close the GDS file! seqClose(seqData) "],["association-tests-part-ii.html", "5 Association tests - Part II 5.1 Sliding window tests 5.2 Exercise 5.3 Solution", " 5 Association tests - Part II These exercises continue the introduction to genetic association testing. Here, we introduce multiple-variant association tests, which are commonly used for testing rare variants in aggregate. 5.1 Sliding window tests We can perform burden, SKAT, SKAT-O, fastSKAT, and SMMAT tests using the GENESIS function assocTestAggregate. First, we need to load the null model and AnnotatedDataFrame (sample annotation + phenotype data) that we created in the previous session, and we need to create our SeqVarData object linking the GDS file to the AnnotatedDataFrame. repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) # load our null model nullmodfile &lt;- &quot;data/null_model.RData&quot; if (!file.exists(nullmodfile)) download.file(file.path(repo_path, nullmodfile), nullmodfile) nullmod &lt;- get(load(nullmodfile)) # load our sample annotation annotfile &lt;- &quot;data/sample_phenotype_annotation.RData&quot; if (!file.exists(annotfile)) download.file(file.path(repo_path, annotfile), annotfile) annot &lt;- get(load(annotfile)) # open the GDS file library(SeqVarTools) gdsfile &lt;- &quot;data/1KG_phase3_subset_chr1.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(repo_path, gdsfile), gdsfile) gdsfmt::showfile.gds(closeall=TRUE) # make sure file is not already open gds &lt;- seqOpen(gdsfile) # make the seqVarData object seqData &lt;- SeqVarData(gds, sampleData=annot) 5.1.1 Burden test First, we perform a burden test. We restrict the test to variants with alternate allele frequency &lt; 0.1. (For real data, this threshold would be lower, perhaps 0.05 or 0.01.) We use a flat weighting scheme – i.e. every variant gets the same weight. We define a sliding window across the genome using a SeqVarWindowIterator object. # make the window iterator object iterator &lt;- SeqVarWindowIterator(seqData, windowSize=10000, windowShift=5000, verbose=FALSE) # run the burden test library(GENESIS) assoc &lt;- assocTestAggregate(iterator, nullmod, test=&quot;Burden&quot;, AF.max=0.1, weight.beta=c(1,1), verbose = FALSE) The function returns the primary results for each window in one table. It also returns a list of tables that contain the variant details for each window tested. names(assoc) ## [1] &quot;results&quot; &quot;variantInfo&quot; # results for each window head(assoc$results) ## chr start end n.site n.alt n.sample.alt Score Score.SE Score.Stat Score.pval Est Est.SE PVE ## 1 1 965001 975000 1 9 9 -0.1191236 0.2577712 -0.4621292 0.643988693 -1.792788 3.879410 0.0001905115 ## 2 1 980001 990000 1 111 107 -1.6707553 0.8841849 -1.8895996 0.058811535 -2.137109 1.130985 0.0031851797 ## 3 1 1020001 1030000 1 1 1 -0.2795838 0.1007173 -2.7759261 0.005504472 -27.561563 9.928781 0.0068740102 ## 4 1 1260001 1270000 1 2 2 -0.1105487 0.1085480 -1.0184319 0.308472744 -9.382319 9.212515 0.0009252485 ## 5 1 1465001 1475000 1 16 16 0.3630992 0.3456555 1.0504657 0.293504065 3.039054 2.893054 0.0009843694 ## 6 1 1730001 1740000 1 5 5 -0.1300405 0.1973175 -0.6590420 0.509868790 -3.340007 5.067973 0.0003874544 # how many variants in each window? table(assoc$results$n.site) ## ## 0 1 2 ## 143 963 16 # variant details for windows with &gt; 1 variant idx &lt;- which(assoc$results$n.site &gt; 1) head(assoc$variantInfo[idx]) ## [[1]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 147 1 27443183 1 1126 0.0013321492 3 1 ## 2 148 1 27448645 1 1126 0.0004440497 1 1 ## ## [[2]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 207 1 41346174 1 1126 0.0004440497 1 1 ## 2 208 1 41352776 1 1126 0.0017761989 4 1 ## ## [[3]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 246 1 51376035 1 1126 0.0004440497 1 1 ## 2 247 1 51381953 1 1126 0.0013321492 3 1 ## ## [[4]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 279 1 57342828 1 1126 0.0062166963 14 1 ## 2 280 1 57344100 1 1126 0.0004440497 1 1 ## ## [[5]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 293 1 59912193 1 1126 0.001776199 4 1 ## 2 294 1 59917976 1 1126 0.003108348 7 1 ## ## [[6]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 356 1 73050313 1 1126 0.0368561279 83 1 ## 2 357 1 73050352 1 1126 0.0008880995 2 1 We can make a QQ plot of the burden p-values from the main results table. library(ggplot2) qqPlot &lt;- function(pval) { pval &lt;- pval[!is.na(pval)] n &lt;- length(pval) x &lt;- 1:n dat &lt;- data.frame(obs=sort(pval), exp=x/n, upper=qbeta(0.025, x, rev(x)), lower=qbeta(0.975, x, rev(x))) ggplot(dat, aes(-log10(exp), -log10(obs))) + geom_line(aes(-log10(exp), -log10(upper)), color=&quot;gray&quot;) + geom_line(aes(-log10(exp), -log10(lower)), color=&quot;gray&quot;) + geom_point() + geom_abline(intercept=0, slope=1, color=&quot;red&quot;) + xlab(expression(paste(-log[10], &quot;(expected P)&quot;))) + ylab(expression(paste(-log[10], &quot;(observed P)&quot;))) + theme_bw() } # make a QQ plot of the burden test p-values qqPlot(assoc$results$Score.pval) 5.1.2 SKAT test We can also perform a SKAT test. This time, we will use the Wu weights, which give larger weights to rarer variants. # reset the iterator to the first window resetIterator(iterator) ## # of selected variants: 1 # run the SKAT test assoc &lt;- assocTestAggregate(iterator, nullmod, test=&quot;SKAT&quot;, AF.max=0.1, weight.beta=c(1,25), verbose = FALSE) # results for each window head(assoc$results) ## chr start end n.site n.alt n.sample.alt Q pval err pval.method ## 1 1 965001 975000 1 9 9 7.318095 0.643988693 0 integration ## 2 1 980001 990000 1 111 107 154.178289 0.058811535 0 integration ## 3 1 1020001 1030000 1 1 1 47.823918 0.005504472 0 integration ## 4 1 1260001 1270000 1 2 2 7.319239 0.308472744 0 integration ## 5 1 1465001 1475000 1 16 16 58.518665 0.293504065 0 integration ## 6 1 1730001 1740000 1 5 5 9.499539 0.509868790 0 integration # variant details for windows with &gt; 1 variant idx &lt;- which(assoc$results$n.site &gt; 1) head(assoc$variantInfo[idx]) ## [[1]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 147 1 27443183 1 1126 0.0013321492 3 24.21284 ## 2 148 1 27448645 1 1126 0.0004440497 1 24.73493 ## ## [[2]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 207 1 41346174 1 1126 0.0004440497 1 24.73493 ## 2 208 1 41352776 1 1126 0.0017761989 4 23.95577 ## ## [[3]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 246 1 51376035 1 1126 0.0004440497 1 24.73493 ## 2 247 1 51381953 1 1126 0.0013321492 3 24.21284 ## ## [[4]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 279 1 57342828 1 1126 0.0062166963 14 21.52488 ## 2 280 1 57344100 1 1126 0.0004440497 1 24.73493 ## ## [[5]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 293 1 59912193 1 1126 0.001776199 4 23.95577 ## 2 294 1 59917976 1 1126 0.003108348 7 23.20016 ## ## [[6]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 356 1 73050313 1 1126 0.0368561279 83 10.15145 ## 2 357 1 73050352 1 1126 0.0008880995 2 24.47255 # make a QQ plot of the SKAT test p-values qqPlot(assoc$results$pval) 5.2 Exercise Perform a sliding window SKAT test for the outcome status. Adjust your model for the covariates sex and study. When performing your SKAT test, use all variants with alternate allele frequency &lt; 20%, and use the Wu weights to give larger weights to rarer variants. Use the same windowSize and windowShift as in the examples. How many windows have &gt;1 variant? Make a QQ plot of the SKAT p-values. 5.3 Solution Perform a sliding window SKAT test for the outcome status. Adjust your model for the covariates sex and study. When performing your SKAT test, use all variants with alternate allele frequency &lt; 20%, and use the Wu weights to give larger weights to rarer variants. Use the same windowSize and windowShift as in the examples. How many windows have &gt;1 variant? Make a QQ plot of the SKAT p-values. The first step is to fit our null model – since our outcome, status, is a binary variable, we must fit a logistic regression null model using the family = binomial argument. The second step is to create our SeqVarWindowIterator object. The third step is to perform the SKAT test using assocTestAggregate – we can set the maximum alternate allele frequency with the AF.max argument, and we can set the variant weights with the weight.beta argument. nullmod.status &lt;- fitNullModel(annot, outcome=&quot;status&quot;, covars=c(&quot;sex&quot;, &quot;study&quot;), family=binomial, verbose=FALSE) seqResetFilter(seqData, verbose=FALSE) iterator &lt;- SeqVarWindowIterator(seqData, windowSize=10000, windowShift=5000, verbose=FALSE) assoc &lt;- assocTestAggregate(iterator, nullmod, test=&quot;SKAT&quot;, AF.max=0.2, weight.beta=c(1,25), verbose = FALSE) # results for each window head(assoc$results) ## chr start end n.site n.alt n.sample.alt Q pval err pval.method ## 1 1 965001 975000 1 9 9 7.318095 0.643988693 0 integration ## 2 1 980001 990000 1 111 107 154.178289 0.058811535 0 integration ## 3 1 1020001 1030000 1 1 1 47.823918 0.005504472 0 integration ## 4 1 1260001 1270000 1 2 2 7.319239 0.308472744 0 integration ## 5 1 1465001 1475000 1 16 16 58.518665 0.293504065 0 integration ## 6 1 1730001 1740000 1 5 5 9.499539 0.509868790 0 integration # how many variants in each window? table(assoc$results$n.site) ## ## 0 1 2 ## 95 1007 20 # variant details for windows with &gt; 1 variant idx &lt;- which(assoc$results$n.site &gt; 1) head(assoc$variantInfo[idx]) ## [[1]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 147 1 27443183 1 1126 0.0013321492 3 24.21284 ## 2 148 1 27448645 1 1126 0.0004440497 1 24.73493 ## ## [[2]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 207 1 41346174 1 1126 0.0004440497 1 24.73493 ## 2 208 1 41352776 1 1126 0.0017761989 4 23.95577 ## ## [[3]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 246 1 51376035 1 1126 0.0004440497 1 24.73493 ## 2 247 1 51381953 1 1126 0.0013321492 3 24.21284 ## ## [[4]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 279 1 57342828 1 1126 0.0062166963 14 21.52488 ## 2 280 1 57344100 1 1126 0.0004440497 1 24.73493 ## ## [[5]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 293 1 59912193 1 1126 0.001776199 4 23.95577 ## 2 294 1 59917976 1 1126 0.003108348 7 23.20016 ## ## [[6]] ## variant.id chr pos allele.index n.obs freq MAC weight ## 1 356 1 73050313 1 1126 0.0368561279 83 10.15145 ## 2 357 1 73050352 1 1126 0.0008880995 2 24.47255 # make a QQ plot of the SKAT test p-values qqPlot(assoc$results$pval) seqClose(seqData) "],["ancestry-and-relatedness-inference.html", "6 Ancestry and Relatedness Inference 6.1 LD-pruning 6.2 Computing a GRM 6.3 De-convoluting ancestry and relatedness 6.4 Exercises 6.5 Solutions", " 6 Ancestry and Relatedness Inference 6.1 LD-pruning We generally advise that population structure and relatedness inference be performed using a set of (nearly) independent genetic variants. To find this set of variants, we perform linkage-disequilibrium (LD) pruning on the study sample set. We typically use an LD threshold of r^2 &lt; 0.1 to select variants. library(SeqArray) repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; # use a GDS file with all chromosomes if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) gdsfile &lt;- &quot;data/1KG_phase3_subset.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(repo_path, gdsfile), gdsfile) gdsfmt::showfile.gds(closeall=TRUE) # make sure file is not already open gds &lt;- seqOpen(gdsfile) # run LD pruning library(SNPRelate) set.seed(100) # LD pruning has a random element; so make this reproducible snpset &lt;- snpgdsLDpruning(gds, method=&quot;corr&quot;, slide.max.bp=10e6, ld.threshold=sqrt(0.1)) ## SNV pruning based on LD: ## Excluding 1,120 SNVs on non-autosomes ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## # of selected variants: 24,387 ## Excluding 253 SNVs (monomorphic: TRUE, MAF: NaN, missing rate: NaN) ## # of samples: 1,126 ## # of SNVs: 24,387 ## using 1 thread ## sliding window: 10,000,000 basepairs, Inf SNPs ## |LD| threshold: 0.316228 ## method: correlation ## Chromosome 1: 92.23%, 1,033/1,120 ## Chromosome 2: 91.16%, 1,021/1,120 ## Chromosome 3: 91.16%, 1,021/1,120 ## Chromosome 4: 91.16%, 1,021/1,120 ## Chromosome 5: 91.88%, 1,029/1,120 ## Chromosome 6: 89.02%, 997/1,120 ## Chromosome 7: 90.54%, 1,014/1,120 ## Chromosome 8: 89.02%, 997/1,120 ## Chromosome 9: 88.66%, 993/1,120 ## Chromosome 10: 86.88%, 973/1,120 ## Chromosome 11: 86.79%, 972/1,120 ## Chromosome 12: 88.30%, 989/1,120 ## Chromosome 13: 86.34%, 967/1,120 ## Chromosome 14: 84.46%, 946/1,120 ## Chromosome 15: 84.11%, 942/1,120 ## Chromosome 16: 83.93%, 940/1,120 ## Chromosome 17: 84.91%, 951/1,120 ## Chromosome 18: 84.46%, 946/1,120 ## Chromosome 19: 81.96%, 918/1,120 ## Chromosome 20: 82.05%, 919/1,120 ## Chromosome 21: 75.09%, 841/1,120 ## Chromosome 22: 76.52%, 857/1,120 ## 21,287 markers are selected in total. # how many variants on each chr? sapply(snpset, length) ## chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 ## 1033 1021 1021 1021 1029 997 1014 997 993 973 972 989 967 946 942 940 951 946 918 919 841 857 # get the full list of LD-pruned variants pruned &lt;- unlist(snpset, use.names=FALSE) length(pruned) ## [1] 21287 6.2 Computing a GRM We can use the SNPRelate package to compute a Genetic Relationship matrix (GRM). A GRM captures genetic relatedness due to both distant ancestry (i.e. population structure) and recent kinship (i.e. family structure) in a single matrix. SNPRelate offers several algorithms for computing a GRM, including the commonly-used GCTA Yang et al 2011. The most recent algorithm added to the package is “IndivBeta” Weir and Goudet 2017. # compute the GRM library(SNPRelate) grm &lt;- snpgdsGRM(gds, method=&quot;GCTA&quot;, snp.id = pruned) ## Genetic Relationship Matrix (GRM, GCTA): ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## # of selected variants: 21,287 ## # of samples: 1,126 ## # of SNVs: 21,287 ## using 1 thread ## CPU capabilities: Double-Precision SSE2 ## Tue Jul 20 17:41:38 2021 (internal increment: 896) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 4s ## Tue Jul 20 17:41:42 2021 Done. names(grm) ## [1] &quot;sample.id&quot; &quot;snp.id&quot; &quot;method&quot; &quot;grm&quot; dim(grm$grm) ## [1] 1126 1126 # look at the top corner of the matrix grm$grm[1:5,1:5] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.72284858 0.03428003 0.02475889 0.04051960 0.02397084 ## [2,] 0.03428003 0.74335217 0.02098193 0.02772272 0.02125452 ## [3,] 0.02475889 0.02098193 0.48389966 0.03247437 0.03382292 ## [4,] 0.04051960 0.02772272 0.03247437 0.80364821 0.02771950 ## [5,] 0.02397084 0.02125452 0.03382292 0.02771950 0.47465797 6.3 De-convoluting ancestry and relatedness To disentangle distant ancestry (i.e. population structure) from recent kinship (i.e. familial relatedness), we implement the analysis described in Conomos et al., 2016. This approach uses the KING, PC-AiR, and PC-Relate methods. 6.3.1 KING Step 1 is to get initial kinship estimates using KING-robust, which is robust to discrete population structure but not ancestry admixture. KING-robust will be able to identify close relatives (e.g. 1st and 2nd degree) reliably, but may identify spurious pairs or miss more distant pairs of relatives in the presence of admixture. KING is available as its own software, but the KING-robust algorithm is also available in SNPRelate. # run KING-robust king &lt;- snpgdsIBDKING(gds, snp.id=pruned) ## IBD analysis (KING method of moment) on genotypes: ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 1s ## # of selected variants: 21,287 ## # of samples: 1,126 ## # of SNVs: 21,287 ## using 1 thread ## No family is specified, and all individuals are treated as singletons. ## Relationship inference in the presence of population stratification. ## CPU capabilities: Double-Precision SSE2 ## Tue Jul 20 17:41:43 2021 (internal increment: 28672) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 1s ## Tue Jul 20 17:41:44 2021 Done. names(king) ## [1] &quot;sample.id&quot; &quot;snp.id&quot; &quot;afreq&quot; &quot;IBS0&quot; &quot;kinship&quot; dim(king$kinship) ## [1] 1126 1126 kingMat &lt;- king$kinship colnames(kingMat) &lt;- rownames(kingMat) &lt;- king$sample.id # look at the top corner of the matrix kingMat[1:5,1:5] ## HG00096 HG00097 HG00099 HG00100 HG00101 ## HG00096 0.500000000 0.013295347 0.021206409 0.003261883 0.021668220 ## HG00097 0.013295347 0.500000000 0.007122507 -0.009021842 0.003561254 ## HG00099 0.021206409 0.007122507 0.500000000 0.037700283 0.020028275 ## HG00100 0.003261883 -0.009021842 0.037700283 0.500000000 0.016992551 ## HG00101 0.021668220 0.003561254 0.020028275 0.016992551 0.500000000 We extract pairwise kinship estimates and IBS0 values (the proportion of variants for which the pair of indivdiuals share 0 alleles identical by state) to plot. We use a hexbin plot to visualize the relatedness for all pairs of samples. kinship &lt;- snpgdsIBDSelection(king) head(kinship) ## ID1 ID2 IBS0 kinship ## 1 HG00096 HG00097 0.007986095 0.013295347 ## 2 HG00096 HG00099 0.007845164 0.021206409 ## 3 HG00096 HG00100 0.008831681 0.003261883 ## 4 HG00096 HG00101 0.007939118 0.021668220 ## 5 HG00096 HG00102 0.008737727 -0.003362152 ## 6 HG00096 HG00103 0.007939118 0.026241799 library(ggplot2) ggplot(kinship, aes(IBS0, kinship)) + geom_hline(yintercept=2^(-seq(3,9,2)/2), linetype=&quot;dashed&quot;, color=&quot;grey&quot;) + geom_hex(bins = 100) + ylab(&quot;kinship estimate&quot;) + theme_bw() We see a few parent-offspring, full sibling, 2nd degree, and 3rd degree relative pairs. The abundance of negative estimates represent pairs of individuals who have ancestry from different populations – the magnitude of the negative relationship is informative of how different their ancestries are; more on this below. 6.3.2 PC-AiR The next step is PC-AiR, which provides robust population structure inference in samples with kinship and pedigree structure. PC-AiR is available in the GENESIS package via the function pcair. First, PC-AiR partitions the full sample set into a set of mutually unrelated samples that is maximally informative about all ancestries in the sample (i.e. the unrelated set) and their relatives (i.e. the related set). We use a 3rd degree kinship threshold (kin.thresh = 2^(-9/2)), which corresponds to first cousins – this defines anyone less related than first cousins as “unrelated”. We use the negative KING-robust estimates as “ancestry divergence” measures (divMat) to identify pairs of samples with different ancestry – we preferentially select individuals with many negative estimates for the unrelated set to ensure ancestry representation. For now, we also use the KING-robust estimates as our kinship measures (kinMat); more on this below. Once the unrelated and related sets are identified, PC-AiR performs a standard Principal Component Analysis (PCA) on the unrelated set of individuals and then projects the relatives onto the PCs. Under the hood, PC-AiR uses the SNPRelate package for efficient PC computation and projection. # run PC-AiR library(GENESIS) pca &lt;- pcair(gds, kinobj = kingMat, kin.thresh=2^(-9/2), divobj = kingMat, div.thresh=-2^(-9/2)) ## Principal Component Analysis (PCA) on genotypes: ## Excluding 1,120 SNVs on non-autosomes ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## # of selected variants: 23,779 ## Excluding 861 SNVs (monomorphic: TRUE, MAF: NaN, missing rate: NaN) ## # of samples: 1,040 ## # of SNVs: 23,779 ## using 1 thread ## # of principal components: 32 ## CPU capabilities: Double-Precision SSE2 ## Tue Jul 20 17:41:45 2021 (internal increment: 972) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 4s ## Tue Jul 20 17:41:49 2021 Begin (eigenvalues and eigenvectors) ## Tue Jul 20 17:41:49 2021 Done. ## SNP Loading: ## # of samples: 1,040 ## # of SNPs: 23,779 ## using 1 thread ## using the top 32 eigenvectors ## Tue Jul 20 17:41:49 2021 (internal increment: 7780) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 1s ## Tue Jul 20 17:41:50 2021 Done. ## Sample Loading: ## # of samples: 86 ## # of SNPs: 23,779 ## using 1 thread ## using the top 32 eigenvectors ## Tue Jul 20 17:41:50 2021 (internal increment: 65536) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## Tue Jul 20 17:41:50 2021 Done. names(pca) ## [1] &quot;vectors&quot; &quot;values&quot; &quot;rels&quot; &quot;unrels&quot; &quot;kin.thresh&quot; &quot;div.thresh&quot; &quot;sample.id&quot; &quot;nsamp&quot; &quot;nsnps&quot; &quot;varprop&quot; &quot;call&quot; &quot;method&quot; # the unrelated set of samples length(pca$unrels) ## [1] 1040 head(pca$unrels) ## [1] &quot;HG00096&quot; &quot;HG00097&quot; &quot;HG00099&quot; &quot;HG00100&quot; &quot;HG00101&quot; &quot;HG00102&quot; # the related set of samples length(pca$rels) ## [1] 86 head(pca$rels) ## [1] &quot;NA18553&quot; &quot;NA19010&quot; &quot;HG00240&quot; &quot;HG01958&quot; &quot;NA18974&quot; &quot;NA19720&quot; # extract the top 10 PCs and make a data.frame pcs &lt;- data.frame(pca$vectors[,1:10]) colnames(pcs) &lt;- paste0(&#39;PC&#39;, 1:10) pcs$sample.id &lt;- pca$sample.id dim(pcs) ## [1] 1126 11 head(pcs) ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 sample.id ## HG00096 -0.02226416 -0.03706223 0.007784659 -0.006036777 -0.0002494770 0.0008316496 -0.03953455 6.079733e-05 0.0009154511 8.333681e-04 HG00096 ## HG00097 -0.02069118 -0.03316550 0.010648323 -0.006725514 0.0004179001 -0.0005008697 -0.03380661 -1.743989e-03 -0.0012662487 -2.869483e-03 HG00097 ## HG00099 -0.02102452 -0.03547247 0.009881201 -0.005776697 0.0005220132 0.0011478104 -0.03409698 6.145952e-07 0.0010956673 -2.258131e-04 HG00099 ## HG00100 -0.02120448 -0.04293781 0.014368290 -0.006435646 0.0004269673 0.0025670278 -0.04708289 1.140219e-03 -0.0007786959 -8.237878e-04 HG00100 ## HG00101 -0.02108916 -0.03594252 0.010240805 -0.003099693 0.0012105916 0.0028715482 -0.03505874 4.599234e-04 0.0068621094 -1.783242e-03 HG00101 ## HG00102 -0.02209371 -0.04086700 0.013113725 -0.001004959 -0.0014970887 -0.0008704542 -0.03083835 -1.097275e-03 0.0030383585 6.466877e-05 HG00102 We’d like to determine which PCs are ancestry informative. To do this we look at the PCs in conjunction with population information for the 1000 Genomes samples. This information is stored in an AnnotatedDataFrame. We make a parallel coordinates plot, color-coding by 1000 Genomes population. library(Biobase) sampfile &lt;- &quot;data/sample_annotation.RData&quot; if (!file.exists(sampfile)) download.file(file.path(repo_path, sampfile), sampfile) annot &lt;- get(load(sampfile)) library(dplyr) annot &lt;- pData(annot) %&gt;% dplyr::select(sample.id, Population) pc.df &lt;- left_join(pcs, annot, by=&quot;sample.id&quot;) library(GGally) library(RColorBrewer) pop.cols &lt;- setNames(brewer.pal(12, &quot;Paired&quot;), c(&quot;ACB&quot;, &quot;ASW&quot;, &quot;CEU&quot;, &quot;GBR&quot;, &quot;CHB&quot;, &quot;JPT&quot;, &quot;CLM&quot;, &quot;MXL&quot;, &quot;LWK&quot;, &quot;YRI&quot;, &quot;GIH&quot;, &quot;PUR&quot;)) ggparcoord(pc.df, columns=1:10, groupColumn=&quot;Population&quot;, scale=&quot;uniminmax&quot;) + scale_color_manual(values=pop.cols) + xlab(&quot;PC&quot;) + ylab(&quot;&quot;) 6.3.3 PC-Relate The next step is PC-Relate, which provides accurate kinship inference, even in the presence of population structure and ancestry admixture, by conditioning on ancestry informative PCs. As we saw above, the first 4 PCs separate populations in our study, so we condition on PCs 1-4 in our PC-Relate analysis. PC-Relate can be performed using the pcrelate function in GENESIS, which expects a SeqVarIterator object for the genotype data. The training.set argument allows for specification of which samples to use to “learn” the ancestry adjustment – we recommend the unrelated set from the PC-AiR analysis. (NOTE: this will take a few minutes to run). seqResetFilter(gds, verbose=FALSE) library(SeqVarTools) seqData &lt;- SeqVarData(gds) # filter the GDS object to our LD-pruned variants seqSetFilter(seqData, variant.id=pruned) ## # of selected variants: 21,287 iterator &lt;- SeqVarBlockIterator(seqData, verbose=FALSE) pcrel &lt;- pcrelate(iterator, pcs=pca$vectors[,1:4], training.set=pca$unrels) names(pcrel) ## [1] &quot;kinBtwn&quot; &quot;kinSelf&quot; # relatedness between pairs of individuals dim(pcrel$kinBtwn) ## [1] 633375 6 head(pcrel$kinBtwn) ## ID1 ID2 kin k0 k2 nsnp ## 1 HG00096 HG00097 0.0195745605 0.8884407 -0.03326108 4238 ## 2 HG00096 HG00099 0.0066373621 0.9415895 -0.03186106 4253 ## 3 HG00096 HG00100 0.0008478914 0.9733286 -0.02327980 4200 ## 4 HG00096 HG00101 0.0094582260 0.9347343 -0.02743277 4259 ## 5 HG00096 HG00102 0.0143016544 0.9578218 0.01502844 4219 ## 6 HG00096 HG00103 0.0168467786 0.9103588 -0.02225413 4252 # self-kinship estimates dim(pcrel$kinSelf) ## [1] 1126 3 head(pcrel$kinSelf) ## ID f nsnp ## 1 HG00096 -0.010207002 4278 ## 2 HG00097 0.003093096 4326 ## 3 HG00099 -0.060436310 4303 ## 4 HG00100 0.012633672 4270 ## 5 HG00101 -0.033837780 4302 ## 6 HG00102 0.030378865 4264 We plot the pairwise kinship estimates againts the IBD0 (k0) estimates (the proportion of variants for which the pair of individuals share 0 alleles identical by descent). We use a hexbin plot to visualize the relatedness for all pairs of samples. ggplot(pcrel$kinBtwn, aes(k0, kin)) + geom_hline(yintercept=2^(-seq(3,9,2)/2), linetype=&quot;dashed&quot;, color=&quot;grey&quot;) + geom_hex(bins = 100) + geom_abline(intercept = 0.25, slope = -0.25) + ylab(&quot;kinship estimate&quot;) + theme_bw() We see that the PC-Relate relatedness estimates for unrelated pairs (i.e. kin ~ 0 and k0 ~ 1) are much closer to expectation than those from KING-robust. We can use the pcrelateToMatrix function to transform the output into an (n x n) kinship matrix (KM). pcrelMat &lt;- pcrelateToMatrix(pcrel, scaleKin=1, verbose=FALSE) dim(pcrelMat) ## [1] 1126 1126 # look at the top corner of the matrix pcrelMat[1:5,1:5] ## 5 x 5 Matrix of class &quot;dsyMatrix&quot; ## HG00096 HG00097 HG00099 HG00100 HG00101 ## HG00096 0.4948964992 0.0195745605 0.0066373621 0.0008478914 0.0094582260 ## HG00097 0.0195745605 0.5015465480 0.0008448096 0.0022230394 0.0048742994 ## HG00099 0.0066373621 0.0008448096 0.4697818452 0.0290478189 -0.0002944969 ## HG00100 0.0008478914 0.0022230394 0.0290478189 0.5063168358 0.0100841545 ## HG00101 0.0094582260 0.0048742994 -0.0002944969 0.0100841545 0.4830811098 6.4 Exercises In small samples (such as this one), we recommend performing a second iteration of PC-AiR and PC-Relate. Now that we have the PC-Relate ancestry-adjusted kinship estimates, we can better partition our sample into unrelated and related sets. This can lead to better ancestry PCs from PC-AiR and better relatedness estimates from PC-Relate. Perform a second PC-AiR analysis, this time using the PC-Relate kinship matrix as the kinship estimates (you should still use the KING-robust matrix for the ancestry divergence estimates). How does the unrelated set compare to the first PC-AiR analysis? Make a parallel coordinates plot of the top 10 PC-AiR PCs. How does this compare to the plot from the first iteration? How many PCs seem to reflect ancestry? Perform a second PC-Relate analysis, this time using the new PC-AiR PCs to adjust for ancestry. Make a hexbin plot of estimated kinship vs IBD0. 6.5 Solutions Perform a second PC-AiR analysis, this time using the PC-Relate kinship matrix as the kinship estimates (you should still use the KING-robust matrix for the ancestry divergence estimates). How does the unrelated set compare to the first PC-AiR analysis? We run the second PC-AiR analysis the same as the first, except using the PC-Relate kinship matrix we created above (pcrelMat) as the input to parameter kinobj – this means that we are using the PC-Relate estimates instead of the KING estimates to identify related pairs of samples. In the solution shown here, we also demonstrate that a SeqVarData object can be used as input, but we need to specify the variants to use in the analysis using the snp.include parameter. seqResetFilter(seqData) ## # of selected samples: 1,126 ## # of selected variants: 25,760 # run PC-AiR pca2 &lt;- pcair(seqData, kinobj=pcrelMat, kin.thresh=2^(-9/2), divobj=kingMat, div.thresh=-2^(-9/2), snp.include = pruned) ## Principal Component Analysis (PCA) on genotypes: ## Calculating allele counts/frequencies ... ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## # of selected variants: 20,945 ## Excluding 342 SNVs (monomorphic: TRUE, MAF: NaN, missing rate: NaN) ## # of samples: 1,070 ## # of SNVs: 20,945 ## using 1 thread ## # of principal components: 32 ## CPU capabilities: Double-Precision SSE2 ## Tue Jul 20 17:45:10 2021 (internal increment: 944) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 4s ## Tue Jul 20 17:45:14 2021 Begin (eigenvalues and eigenvectors) ## Tue Jul 20 17:45:15 2021 Done. ## SNP Loading: ## # of samples: 1,070 ## # of SNPs: 20,945 ## using 1 thread ## using the top 32 eigenvectors ## Tue Jul 20 17:45:15 2021 (internal increment: 7564) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## Tue Jul 20 17:45:15 2021 Done. ## Sample Loading: ## # of samples: 56 ## # of SNPs: 20,945 ## using 1 thread ## using the top 32 eigenvectors ## Tue Jul 20 17:45:15 2021 (internal increment: 65536) ## [..................................................] 0%, ETC: --- [==================================================] 100%, completed, 0s ## Tue Jul 20 17:45:15 2021 Done. names(pca2) ## [1] &quot;vectors&quot; &quot;values&quot; &quot;rels&quot; &quot;unrels&quot; &quot;kin.thresh&quot; &quot;div.thresh&quot; &quot;sample.id&quot; &quot;nsamp&quot; &quot;nsnps&quot; &quot;varprop&quot; &quot;call&quot; &quot;method&quot; # the unrelated set of samples length(pca2$unrels) ## [1] 1070 # the related set of samples length(pca2$rels) ## [1] 56 # extract the top 10 PCs and make a data.frame pcs2 &lt;- data.frame(pca2$vectors[,1:10]) colnames(pcs2) &lt;- paste0(&#39;PC&#39;, 1:10) pcs2$sample.id &lt;- pca2$sample.id dim(pcs2) ## [1] 1126 11 save(pcs2, file=&quot;data/pcs.RData&quot;) We see that there are now 1,070 samples in the unrelated set, as opposed to 1,040 in the first PC-AiR analysis. This indicates that KING-robust likely overestimated relatedness for some pairs due to bias from ancestry admixture. Make a parallel coordinates plot of the top 10 PC-AiR PCs. How does this compare to the plot from the first iteration? How many PCs seem to reflect ancestry? We can reuse the code from above to make the parallel coordinates plot. library(Biobase) sampfile &lt;- &quot;data/sample_annotation.RData&quot; if (!file.exists(sampfile)) download.file(file.path(repo_path, sampfile), sampfile) annot &lt;- get(load(sampfile)) pc.df &lt;- as.data.frame(pcs2) names(pc.df) &lt;- 1:ncol(pcs2) pc.df$sample.id &lt;- row.names(pcs2) library(dplyr) annot &lt;- pData(annot) %&gt;% dplyr::select(sample.id, Population) pc.df &lt;- left_join(pcs2, annot, by=&quot;sample.id&quot;) library(GGally) library(RColorBrewer) pop.cols &lt;- setNames(brewer.pal(12, &quot;Paired&quot;), c(&quot;ACB&quot;, &quot;ASW&quot;, &quot;CEU&quot;, &quot;GBR&quot;, &quot;CHB&quot;, &quot;JPT&quot;, &quot;CLM&quot;, &quot;MXL&quot;, &quot;LWK&quot;, &quot;YRI&quot;, &quot;GIH&quot;, &quot;PUR&quot;)) ggparcoord(pc.df, columns=1:10, groupColumn=&quot;Population&quot;, scale=&quot;uniminmax&quot;) + scale_color_manual(values=pop.cols) + xlab(&quot;PC&quot;) + ylab(&quot;&quot;) The plot looks a bit cleaner than the one from the first PC-AiR analysis. Clearly, PCs 1-4 are reflective of ancestry here. In the prior analysis, PC7 also seemed to pick up some component of ancestry. Perform a second PC-Relate analysis, this time using the new PC-AiR PCs to adjust for ancestry. Make a hexbin plot of estimated kinship vs IBD0. We run the second PC-Relate analysis the same as the first, except using the new PC-AiR PCs that we just generated to adjust for ancestry, and using the new PC-AiR unrelated set as our training.set. # filter the GDS object to our LD-pruned variants seqSetFilter(seqData, variant.id=pruned) ## # of selected variants: 21,287 iterator &lt;- SeqVarBlockIterator(seqData, verbose=FALSE) # run PC-Relate pcrel2 &lt;- pcrelate(iterator, pcs=pca2$vectors[,1:4], training.set=pca2$unrels) save(pcrel2, file=&quot;data/pcrelate_kinship.RData&quot;) ggplot(pcrel2$kinBtwn, aes(k0, kin)) + geom_hline(yintercept=2^(-seq(3,9,2)/2), linetype=&quot;dashed&quot;, color=&quot;grey&quot;) + geom_hex(bins = 100) + geom_abline(intercept = 0.25, slope = -0.25) + ylab(&quot;kinship estimate&quot;) + theme_bw() seqClose(seqData) "],["mixed-models.html", "7 Mixed models 7.1 Null model 7.2 Single-variant tests 7.3 Exercise 7.4 Solution", " 7 Mixed models These exercises extend what was previously introduced in the association tests from regression models to mixed models that account for genetic relatedness among samples. 7.1 Null model Recall that the first step in our association testing procedure is to fit the null model. In addition to the AnnotatedDataFrame with phenotype data that we used previously, we will also use the ancestry PCs and pairwise kinship estimates we created in the previous session. We will use the first 4 PCs to adjust for ancestry. # sample annotation repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) sampfile &lt;- &quot;data/sample_phenotype_annotation.RData&quot; if (!file.exists(sampfile)) download.file(file.path(repo_path, sampfile), sampfile) annot &lt;- get(load(sampfile)) library(Biobase) head(pData(annot)) ## sample.id subject.id Population Population.Description sex status age height study ## 1 HG00096 HG00096 GBR British in England and Scotland M 0 47 165.300 study_1 ## 2 HG00097 HG00097 GBR British in England and Scotland F 1 47 144.780 study_3 ## 3 HG00099 HG00099 GBR British in England and Scotland F 0 40 185.500 study_2 ## 4 HG00100 HG00100 GBR British in England and Scotland F 1 45 150.622 study_3 ## 5 HG00101 HG00101 GBR British in England and Scotland M 0 40 177.800 study_3 ## 6 HG00102 HG00102 GBR British in England and Scotland F 0 49 169.100 study_1 # load the ancestry PCs pcfile &lt;- &quot;data/pcs.RData&quot; if (!file.exists(pcfile)) download.file(file.path(repo_path, pcfile), pcfile) pcs &lt;- get(load(pcfile)) pcs &lt;- pcs[,c(&quot;sample.id&quot;, &quot;PC1&quot;, &quot;PC2&quot;, &quot;PC3&quot;, &quot;PC4&quot;)] head(pcs) ## sample.id PC1 PC2 PC3 PC4 ## HG00096 HG00096 -0.02189121 -0.03637024 -0.01328390 0.005533490 ## HG00097 HG00097 -0.02002924 -0.03285711 -0.01207786 0.005390183 ## HG00099 HG00099 -0.02088688 -0.03330825 -0.01510526 0.004499776 ## HG00100 HG00100 -0.02031270 -0.03975678 -0.01908062 0.004291604 ## HG00101 HG00101 -0.02038482 -0.03437442 -0.01328870 0.002310498 ## HG00102 HG00102 -0.02144725 -0.03970277 -0.01391896 -0.001245155 # merge PCs with the sample annotation library(dplyr) dat &lt;- left_join(pData(annot), pcs, by=&quot;sample.id&quot;) # update the AnnotatedDataFrame pData(annot) &lt;- dat save(annot, file=&quot;data/sample_phenotype_pcs.RData&quot;) We can create a kinship matrix from the output of pcrelate. We multiply the kinship values by 2 to get values on the same scale as the standard GRM. This matrix is represented in R as a symmetric matrix object from the Matrix package. kinfile &lt;- &quot;data/pcrelate_kinship.RData&quot; if (!file.exists(kinfile)) download.file(file.path(repo_path, kinfile), kinfile) pcrel &lt;- get(load(kinfile)) library(GENESIS) kinship &lt;- pcrelateToMatrix(pcrel, scaleKin=2, verbose=FALSE) dim(kinship) ## [1] 1126 1126 kinship[1:5,1:5] ## 5 x 5 Matrix of class &quot;dsyMatrix&quot; ## HG00096 HG00097 HG00099 HG00100 HG00101 ## HG00096 0.9906231740 0.036922756 0.010409075 0.0005780419 0.016522827 ## HG00097 0.0369227560 1.001560563 -0.000653982 0.0035158297 0.007678616 ## HG00099 0.0104090751 -0.000653982 0.937748649 0.0556213201 -0.003235074 ## HG00100 0.0005780419 0.003515830 0.055621320 1.0111688406 0.017959707 ## HG00101 0.0165228269 0.007678616 -0.003235074 0.0179597069 0.965028548 When running a mixed model analysis, we still fit the null model using the fitNullModel function in GENESIS. Now, we include the kinship matrix in the model with the cov.mat argument, which is used to specify the random effect(s) in the model with covariance structure(s) proportional to the supplied matrix(s). The inclusion of these random effects is what makes this a mixed model, rather than a simple regression model. We also add the ancestry PCs to the list of covariates and allow for heterogeneous residual variance by study with the group.var argument, as before. nullmod &lt;- fitNullModel(annot, outcome=&quot;height&quot;, covars=c(&quot;sex&quot;, &quot;age&quot;, &quot;study&quot;, paste0(&quot;PC&quot;, 1:4)), cov.mat=kinship, group.var=&quot;study&quot;, verbose=FALSE) save(nullmod, file=&quot;data/null_mixed_model.RData&quot;) We can investigate the output from fitNullModel. # description of the model we fit nullmod$model ## $hetResid ## [1] TRUE ## ## $family ## ## Family: gaussian ## Link function: identity ## ## ## $outcome ## [1] &quot;height&quot; ## ## $covars ## [1] &quot;sex&quot; &quot;age&quot; &quot;study&quot; &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## ## $formula ## [1] &quot;height ~ sex + age + study + PC1 + PC2 + PC3 + PC4 + (1|A) + var(study)&quot; # fixed effect regression estimates nullmod$fixef ## Est SE Stat pval ## (Intercept) 163.74431361 3.15130038 2.699932e+03 0.000000e+00 ## sexM 6.27072759 0.67911122 8.526171e+01 2.613754e-20 ## age 0.07420444 0.06839188 1.177201e+00 2.779265e-01 ## studystudy_2 10.54243352 0.81122848 1.688867e+02 1.295192e-38 ## studystudy_3 -8.95546858 0.83932675 1.138451e+02 1.408933e-26 ## PC1 0.05117574 11.25310400 2.068158e-05 9.963715e-01 ## PC2 -61.24485338 11.22450534 2.977178e+01 4.860157e-08 ## PC3 -8.81351399 11.09566802 6.309447e-01 4.270090e-01 ## PC4 -5.61519972 11.19652394 2.515151e-01 6.160103e-01 # variance component estimates by group.var nullmod$varComp ## V_A V_study_1 V_study_3 V_study_2 ## 0.00000 93.61147 168.76021 152.37404 # model fit: fitted values, residuals head(nullmod$fit) ## outcome workingY fitted.values resid.marginal resid.conditional linear.predictor resid.PY resid.cholesky sample.id ## HG00096 165.300 165.300 175.8150 -10.5150254 -10.5150254 175.8150 -0.112326247 -1.08679039 HG00096 ## HG00097 144.780 144.780 160.3639 -15.5839388 -15.5839388 160.3639 -0.092343679 -1.19961587 HG00097 ## HG00099 185.500 185.500 179.4017 6.0983220 6.0983220 179.4017 0.040022053 0.49403175 HG00099 ## HG00100 150.622 150.622 160.7060 -10.0839725 -10.0839725 160.7060 -0.059753258 -0.77624108 HG00100 ## HG00101 177.800 177.800 166.2361 11.5638906 11.5638906 166.2361 0.068522613 0.89016178 HG00101 ## HG00102 169.100 169.100 169.9405 -0.8404905 -0.8404905 169.9405 -0.008978499 -0.08686969 HG00102 library(ggplot2) ggplot(nullmod$fit, aes(x = fitted.values, y = resid.marginal)) + geom_point(alpha = 0.5) + geom_hline(yintercept = 0) + geom_smooth(method = &#39;lm&#39;) 7.2 Single-variant tests Now we can run a single-variant test, accounting for genetic ancestry and genetic relatedness among the subjects. We use the same assocTestSingle function as before; the only difference is that we pass in our new null model. library(SeqVarTools) gdsfile &lt;- &quot;data/1KG_phase3_subset_chr1.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(repo_path, gdsfile), gdsfile) gdsfmt::showfile.gds(closeall=TRUE) # make sure file is not already open gds &lt;- seqOpen(gdsfile) # make the seqVarData object seqData &lt;- SeqVarData(gds, sampleData=annot) # make the iterator object iterator &lt;- SeqVarBlockIterator(seqData, verbose=FALSE) # run the single-variant association test assoc &lt;- assocTestSingle(iterator, nullmod) ## # of selected samples: 1,126 dim(assoc) ## [1] 1129 14 head(assoc) ## variant.id chr pos allele.index n.obs freq MAC Score Score.SE Score.Stat Score.pval Est Est.SE PVE ## 1 1 1 970546 1 1126 0.0039964476 9 -0.17384025 0.2592275 -0.6706088 0.502469798 -2.586950 3.857615 0.0004026103 ## 2 2 1 985900 1 1126 0.0492895204 111 -1.55598468 0.8183596 -1.9013460 0.057256708 -2.323363 1.221957 0.0032364488 ## 3 3 1 1025045 1 1126 0.0004440497 1 -0.29433530 0.1029724 -2.8583911 0.004257953 -27.758817 9.711343 0.0073145856 ## 4 4 1 1265550 1 1126 0.0008880995 2 -0.09940909 0.1078116 -0.9220633 0.356495581 -8.552545 9.275442 0.0007611458 ## 5 5 1 1472676 1 1126 0.0071047957 16 0.14878210 0.3468438 0.4289599 0.667952399 1.236752 2.883142 0.0001647327 ## 6 6 1 1735725 1 1126 0.0022202487 5 -0.13567249 0.1982518 -0.6843443 0.493757749 -3.451895 5.044091 0.0004192719 We make the usual QQ plot, filtering to variants with minor allele count (MAC) &gt;= 5. library(ggplot2) qqPlot &lt;- function(pval) { pval &lt;- pval[!is.na(pval)] n &lt;- length(pval) x &lt;- 1:n dat &lt;- data.frame(obs=sort(pval), exp=x/n, upper=qbeta(0.025, x, rev(x)), lower=qbeta(0.975, x, rev(x))) ggplot(dat, aes(-log10(exp), -log10(obs))) + geom_line(aes(-log10(exp), -log10(upper)), color=&quot;gray&quot;) + geom_line(aes(-log10(exp), -log10(lower)), color=&quot;gray&quot;) + geom_point() + geom_abline(intercept=0, slope=1, color=&quot;red&quot;) + xlab(expression(paste(-log[10], &quot;(expected P)&quot;))) + ylab(expression(paste(-log[10], &quot;(observed P)&quot;))) + theme_bw() } qqPlot(assoc$Score.pval[assoc$MAC &gt;= 5]) Notice that we observe much less inflation than before, when we did not adjust for ancestry and relatedness. 7.3 Exercise Perform a single-variant association test for status. Adjust for sex, age, study, ancestry, and kinship in the model. Don’t forget to consider the family parameter. Make a QQ plot of the p-values for all variants with MAC &gt;= 5. 7.4 Solution Perform a single-variant association test for status. Adjust for sex, age, study, ancestry, and kinship in the model. Don’t forget to consider the family parameter. Make a QQ plot of the p-values for all variants with MAC &gt;= 5. The first step is to fit the null model. We include the first 4 PCs as covariates in our model to adjust for ancestry, and we include a random effect proportional to the kinship matrix to adjust for genetic relatedness. Recall that with a binary outcome, we set family = binomial – because we have a random effect, this will fit an approximate logistic mixed model using the GMMAT method. nullmod.status &lt;- fitNullModel(annot, outcome=&quot;status&quot;, covars=c(&quot;sex&quot;, &quot;age&quot;, &quot;study&quot;, paste0(&quot;PC&quot;, 1:4)), cov.mat=kinship, family = binomial, verbose=FALSE) # description of the model we fit nullmod.status$model ## $hetResid ## [1] FALSE ## ## $family ## ## Family: binomial ## Link function: logit ## ## ## $outcome ## [1] &quot;status&quot; ## ## $covars ## [1] &quot;sex&quot; &quot;age&quot; &quot;study&quot; &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## ## $formula ## [1] &quot;status ~ sex + age + study + PC1 + PC2 + PC3 + PC4 + (1|A)&quot; # fixed effect regression estimates nullmod.status$fixef ## Est SE Stat pval ## (Intercept) -3.182182913 0.96998026 1.076278e+01 0.001035617 ## sexM 0.124463598 0.20598679 3.650951e-01 0.545690084 ## age 0.019985093 0.02075004 9.276289e-01 0.335479807 ## studystudy_2 0.003063956 0.24852285 1.519961e-04 0.990163394 ## studystudy_3 -0.145204949 0.25448616 3.255626e-01 0.568283747 ## PC1 -3.074068322 3.46894981 7.852917e-01 0.375527625 ## PC2 5.076675586 3.21042684 2.500537e+00 0.113807493 ## PC3 -0.236916365 3.45512128 4.701793e-03 0.945332199 ## PC4 1.946715491 3.41350477 3.252400e-01 0.568475488 # variance component estimates by group.var nullmod.status$varComp ## V_A ## 0.2923299 Now that we have the null model, we perform the single-variant association tests and make the QQ plot the same way as before. resetIterator(iterator, verbose=FALSE) # run the single-variant association test assoc.status &lt;- assocTestSingle(iterator, nullmod.status, test=&quot;Score&quot;) ## # of selected samples: 1,126 dim(assoc.status) ## [1] 1129 14 head(assoc.status) ## variant.id chr pos allele.index n.obs freq MAC Score Score.SE Score.Stat Score.pval Est Est.SE PVE ## 1 1 1 970546 1 1126 0.0039964476 9 0.1975748 0.8349756 0.2366235 0.81294892 0.2833897 1.1976398 6.097484e-05 ## 2 2 1 985900 1 1126 0.0492895204 111 -2.5741449 2.6188824 -0.9829173 0.32564813 -0.3753194 0.3818423 1.052129e-03 ## 3 3 1 1025045 1 1126 0.0004440497 1 -0.0721219 0.2555958 -0.2821717 0.77781189 -1.1039760 3.9124270 8.670854e-05 ## 4 4 1 1265550 1 1126 0.0008880995 2 0.7862901 0.4237118 1.8557191 0.06349361 4.3796725 2.3600946 3.750244e-03 ## 5 5 1 1472676 1 1126 0.0071047957 16 0.6300877 1.0837470 0.5813974 0.56097266 0.5364696 0.9227246 3.681131e-04 ## 6 6 1 1735725 1 1126 0.0022202487 5 -0.4029747 0.5941659 -0.6782191 0.49763280 -1.1414641 1.6830315 5.009278e-04 qqPlot(assoc.status$Score.pval[assoc.status$MAC &gt;= 5]) seqClose(seqData) "],["variant-annotation.html", "8 Variant annotation 8.1 Use cases 8.2 Exercise 8.3 Solution", " 8 Variant annotation In this session we will learn how to use Annotation Explorer, an open tool available on NHLBI BioData Catalyst cloud platform that eliminates the challenges of working with very large variant-level annotated datasets. Using Annotation Explorer we will learn how to explore and interactively query variant annotations and integrate them in GWAS analyses. Annotation Explorer can be used pre-association testing, for example to generate annotation informed variant filtering and grouping files for aggregate testing; as well as for post-association testing, for example to explore annotations of variants in a novel GWAS signal. We will execute three representative use cases to demonstrate both pre- and post-GWAS applications. For all the use cases we will be using the open-access datasetTOPMed_freeze5_open, which everyone will have access to. Annotation explorer has interactive graphical user interface built on high performance databases and does not require any programming experience. It currently caps the number of users at a given time and hence we all will not be able to use it live at the same time during the workshop. We request that users perform the hands-on exercises involving Annotation explorer after the workshop is over, at their own convenience. We have provided a detailed tutorial and will provide a video recording of the demo for how to perform the following exercises using Annotation Explorer. 8.1 Use cases 8.1.1 Use case 1 User wants to generate aggregation units for rare variant association testing such that only missense variants which have CADD phred score &gt;20 are grouped by Ensemble gene definitions 8.1.2 Use case 2 User wants to generate aggregation units for rare variant association testing such that they retain only variants with fathmm_MKL_non_coding_score &gt;0.5 and they are grouped by user-defined genomic co-ordinates (example using ATAC-Seq peaks from the tissue of your choice) 8.1.3 Use case 3 User wants to explore the annotations for a variant of their interest 8.2 Exercise Generate a new set of aggregation units by setting up the same filtering criteria as in use case 1 but this time use a different CADD phred score cut-off (example 40, 10) and study how that changes plots under the interactive plots tab of Annotation Explorer. For example, how does changing the filtering criteria change the number of aggregation units with no variants. How does the distribution and number of aggregation units in each bin change in the histogram? 8.3 Solution In general a more stringent filtering approach will reduce the number of aggregation units which have at least one variant (for example you will see fewer units with no variants at CADD phred cut-off 10 vs 40. The change in the distribution of histogram that shows the total number of aggregation units (Y-axis) in the each of bins with varying variant number ranges (X-axis) depends on the characteristics of the features used for grouping criteria (size of the aggregating regions) and distribution of values of the annotation field used for filtering. Sometimes there is not a obvious or recommended cut-off to implement for a annotation field. Playing with varying filtering criteria can help an user visualize its effects on the aggregation unit characteristics and may assist them in choosing a filtering criteria in an informed way "],["annotation-informed-aggregate-association-tests.html", "9 Annotation informed aggregate association tests 9.1 Aggregate unit for association testing exercise 9.2 Association testing with aggregate units 9.3 Exercise 9.4 Solution", " 9 Annotation informed aggregate association tests 9.1 Aggregate unit for association testing exercise Now that we know how to make genome annotation informed aggregation units using Annotation Explorer, such as the gene-based variant aggregation units, we can proceed to an association testing exercise. NOTE : The exercises in this workshop are based on the 1000 genomes dataset mapped to genome build GRCh37/hg19. Because the aggregation units we generated using the Annotation Explorer in the previous section are mapped to GRCh38 and are not based on 1000 genomes data, we will not be using them in this section. Instead, in this exercise we will be using pre-computed aggregation units based on 1000 genomes mapped to GRCh37 so that the annotation positions are consistent with the build used for genotyping data in the workshop. These gene-based units include SNVs from all chromosomes (no indels, and not just chromosome 1 as before). Each genic unit was specified to include the set of SNVs falling within a GENCODE-defined gene boundaries and the 20 kb flanking regions upstream and downstream of that range. This set of aggregation units is not filtered by CADD score or consequence. The aggregation units are defined in an R dataframe in the format consistent with the output from Annotation Explorer and compatible with the GENESIS association testing workflows. Each row of the dataframe specifies a variant (chr, pos, ref, alt) and the group identifier (group_id) it is a part of. Multiple rows with different group identifiers can be specified to assign a variant to different groups (i.e., a variant can be assigned to multiple genes). Begin by loading the aggregation units: library(dplyr) repo_path &lt;- &quot;https://github.com/UW-GAC/SISG_2021/raw/master&quot; if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) aggfile &lt;- &quot;data/variants_by_gene.RData&quot; if (!file.exists(aggfile)) download.file(file.path(repo_path, aggfile), aggfile) aggunit &lt;- get(load(aggfile)) head(aggunit) ## # A tibble: 6 x 5 ## group_id chr pos ref alt ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ENSG00000131591.13 1 1025045 C T ## 2 ENSG00000169962.4 1 1265550 C T ## 3 ENSG00000205090.4 1 1472676 T C ## 4 ENSG00000171603.12 1 9788518 G A ## 5 ENSG00000204624.6 1 11593461 C T ## 6 ENSG00000270914.1 1 12068870 G A # an example of a variant that is present in multiple groups mult &lt;- aggunit %&gt;% group_by(chr, pos) %&gt;% summarise(n=n()) %&gt;% filter(n &gt; 1) inner_join(aggunit, mult[2,1:2]) ## # A tibble: 2 x 5 ## group_id chr pos ref alt ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 ENSG00000187952.8 1 21742183 G A ## 2 ENSG00000227001.2 1 21742183 G A 9.2 Association testing with aggregate units We can run burden and SKAT tests on each of these units using the same assocTestAggregate function we used previously. We define a SeqVarListIterator object where each list element is an aggregate unit. The constructor expects a GRangesList, so we use the TopmedPipeline function aggregateGRangesList to quickly convert our single dataframe to the required format. This function can account for multiallelic variants (the same chromosome, position, and ref, but different alt alleles). # open the GDS file library(SeqVarTools) gdsfile &lt;- &quot;data/1KG_phase3_subset_chr1.gds&quot; if (!file.exists(gdsfile)) download.file(file.path(repo_path, gdsfile), gdsfile) gdsfmt::showfile.gds(closeall=TRUE) # make sure file is not already open gds &lt;- seqOpen(gdsfile) # sample annotation file annotfile &lt;- &quot;data/sample_phenotype_pcs.RData&quot; if (!file.exists(annotfile)) download.file(file.path(repo_path, annotfile), aggfile) annot &lt;- get(load(annotfile)) # make the seqVarData object seqData &lt;- SeqVarData(gds, sampleData=annot) # subset to chromosome 1 aggunit1 &lt;- filter(aggunit, chr == 1) # create the GRangesList object library(TopmedPipeline) aggVarList &lt;- aggregateGRangesList(aggunit1) length(aggVarList) ## [1] 127 head(names(aggVarList)) ## [1] &quot;ENSG00000131591.13&quot; &quot;ENSG00000169962.4&quot; &quot;ENSG00000205090.4&quot; &quot;ENSG00000171603.12&quot; &quot;ENSG00000204624.6&quot; &quot;ENSG00000270914.1&quot; aggVarList[[1]] ## GRanges object with 1 range and 2 metadata columns: ## seqnames ranges strand | ref alt ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;character&gt; &lt;character&gt; ## [1] 1 1025045 * | C T ## ------- ## seqinfo: 23 sequences from an unspecified genome; no seqlengths # construct the iterator using the SeqVarListIterator function iterator &lt;- SeqVarListIterator(seqData, variantRanges=aggVarList, verbose=FALSE) As in the previous section, we must load the null model we fit earlier before running the association test. # load the null model nullmodfile &lt;- &quot;data/null_mixed_model.RData&quot; if (!file.exists(nullmodfile)) download.file(file.path(repo_path, nullmodfile), nullmodfile) nullmod &lt;- get(load(nullmodfile)) # run the burden test library(GENESIS) assoc &lt;- assocTestAggregate(iterator, nullmod, test=&quot;Burden&quot;, AF.max=0.1, weight.beta=c(1,1)) ## # of selected samples: 1,126 names(assoc) ## [1] &quot;results&quot; &quot;variantInfo&quot; head(assoc$results) ## n.site n.alt n.sample.alt Score Score.SE Score.Stat Score.pval Est Est.SE PVE ## ENSG00000131591.13 1 1 1 -0.294335296 0.10297237 -2.85839107 0.004257953 -27.7588166 9.711343 7.314586e-03 ## ENSG00000169962.4 1 2 2 -0.099409087 0.10781157 -0.92206326 0.356495581 -8.5525446 9.275442 7.611458e-04 ## ENSG00000205090.4 1 16 16 0.148782099 0.34684383 0.42895991 0.667952399 1.2367523 2.883142 1.647327e-04 ## ENSG00000171603.12 1 2 2 0.003544954 0.11065596 0.03203582 0.974443489 0.2895083 9.037019 9.187937e-07 ## ENSG00000204624.6 1 1 1 -0.023265336 0.07677702 -0.30302476 0.761870996 -3.9468160 13.024731 8.220584e-05 ## ENSG00000270914.1 1 14 14 -0.775650027 0.32814006 -2.36377727 0.018089684 -7.2035619 3.047479 5.002183e-03 head(names(assoc$variantInfo)) ## [1] &quot;ENSG00000131591.13&quot; &quot;ENSG00000169962.4&quot; &quot;ENSG00000205090.4&quot; &quot;ENSG00000171603.12&quot; &quot;ENSG00000204624.6&quot; &quot;ENSG00000270914.1&quot; assoc$variantInfo[[3]] ## variant.id chr pos ref alt allele.index n.obs freq MAC weight ## 1 5 1 1472676 T C 1 1126 0.007104796 16 1 We can make our usual QQ plot library(ggplot2) qqPlot &lt;- function(pval) { pval &lt;- pval[!is.na(pval)] n &lt;- length(pval) x &lt;- 1:n dat &lt;- data.frame(obs=sort(pval), exp=x/n, upper=qbeta(0.025, x, rev(x)), lower=qbeta(0.975, x, rev(x))) ggplot(dat, aes(-log10(exp), -log10(obs))) + geom_line(aes(-log10(exp), -log10(upper)), color=&quot;gray&quot;) + geom_line(aes(-log10(exp), -log10(lower)), color=&quot;gray&quot;) + geom_point() + geom_abline(intercept=0, slope=1, color=&quot;red&quot;) + xlab(expression(paste(-log[10], &quot;(expected P)&quot;))) + ylab(expression(paste(-log[10], &quot;(observed P)&quot;))) + theme_bw() } qqPlot(assoc$results$Score.pval) 9.3 Exercise Since we are working with a subset of the data, many of the genes listed in group_id have a very small number of variants. Create a new set of aggregation units based on position, rather than gene name – create 10 units that are 1MB long and span all of the chr1 variants by using the TopmedPipeline function aggregateGRanges. Run a SKAT test using those units and a SeqVarRangeIterator object. 9.4 Solution Since we are working with a subset of the data, many of the genes listed in group_id have a very small number of variants. Create a new set of aggregation units based on position, rather than gene name – create 10 units that are 1MB long and span all of the chr1 variants by using the TopmedPipeline function aggregateGRanges. Run a SKAT test with the Wu weights using those units and a SeqVarRangeIterator object. # minimum variant position minp &lt;- min(aggunit1$pos) # maximum variant position maxp &lt;- max(aggunit1$pos) # create a data frame breaking the position range into 10 pieces aggByPos &lt;- data.frame(chr=1, start=seq(minp, maxp-1e6, length.out=10), end=seq(minp+1e6, maxp, length.out=10)) aggByPos$group_id &lt;- 1:nrow(aggByPos) dim(aggByPos) ## [1] 10 4 head(aggByPos) ## chr start end group_id ## 1 1 1025045 2025045 1 ## 2 1 28440219 29440219 2 ## 3 1 55855393 56855393 3 ## 4 1 83270568 84270568 4 ## 5 1 110685742 111685742 5 ## 6 1 138100916 139100916 6 aggVarList &lt;- aggregateGRanges(aggByPos) aggVarList ## GRanges object with 10 ranges and 0 metadata columns: ## seqnames ranges strand ## &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; ## 1 1 1025045-2025045 * ## 2 1 28440219-29440219 * ## 3 1 55855393-56855393 * ## 4 1 83270567-84270567 * ## 5 1 110685741-111685741 * ## 6 1 138100916-139100916 * ## 7 1 165516090-166516090 * ## 8 1 192931264-193931264 * ## 9 1 220346438-221346438 * ## 10 1 247761613-248761613 * ## ------- ## seqinfo: 1 sequence from an unspecified genome; no seqlengths seqResetFilter(seqData, verbose=FALSE) iterator &lt;- SeqVarRangeIterator(seqData, variantRanges=aggVarList, verbose=FALSE) assoc &lt;- assocTestAggregate(iterator, nullmod, test=&quot;SKAT&quot;, AF.max=0.1, weight.beta=c(1,25)) ## # of selected samples: 1,126 assoc$results ## n.site n.alt n.sample.alt Q pval err pval.method ## 1 4 24 24 79.08763 0.3995046 0 saddlepoint ## 2 4 152 137 80.38214 0.4753546 0 saddlepoint ## 3 2 32 32 19.09239 0.6708608 0 saddlepoint ## 4 5 80 76 138.54683 0.4490908 0 saddlepoint ## 5 0 0 0 NA NA NA &lt;NA&gt; ## 6 0 0 0 NA NA NA &lt;NA&gt; ## 7 3 65 62 310.46293 0.1278045 0 saddlepoint ## 8 4 18 18 92.32706 0.2701244 0 saddlepoint ## 9 6 11 11 29.61999 0.5637511 0 integration ## 10 4 33 33 155.50853 0.2035247 0 saddlepoint seqClose(seqData) "],["running-a-gwas-workflow-on-biodata-catalyst.html", "10 Running a GWAS workflow on BioData Catalyst 10.1 Preparing the genotype data 10.2 Ancestry and Relatedness 10.3 Association Testing 10.4 Analysis follow-up", " 10 Running a GWAS workflow on BioData Catalyst On the BioData Catalyst platform, locate the following public apps and copy them to your project. To find the apps, go to Public Gallery &gt; Apps, and then use the search box. Click “copy”, select your project from the dropdown, and click “copy” again. Bcftools Merge and Filter VCF to GDS Converter KING robust PC-AiR PC-Relate GENESIS Null Model GENESIS Single Variant Association Testing GENESIS Aggregate Association Testing 10.1 Preparing the genotype data 10.1.1 Bcftools Merge and Filter Run the Bcftools Merge and Filter tool to combine two separate VCF files into one merged file (see the task “1. Bcftools merge chr 1 subsets”). Inputs Input variant files: 1KG_phase3_chr1.subset1.vcf and 1KG_phase3_chr1.subset2.vcf App Settings Output name: 1KG_phase3_subset_chr1 This will create a file named 1KG_phase3_subset_chr1.merged.vcf.gz that contains all of the data from both input files. 10.1.2 VCF to GDS Converter Run the VCF to GDS Converter workflow to convert the 1000 Genomes VCF file you just created to a GDS file (see the task “2. Convert chr 1 VCF to GDS”). Inputs Variants Files: 1KG_phase3_subset_chr1.merged.vcf.gz App Settings check GDS: No This will create a GDS file named 1KG_phase3_subset_chr1.merged.gds that contains the same information as the input VCF file. Use the “view stats and logs” button to check on the status of your tasks. 10.2 Ancestry and Relatedness 10.2.1 KING robust Run the KING robust workflow to compute initial kinship estimates for all pairs of samples (see the task “3. KING robust run”). Inputs GDS file: 1KG_phase3_subset_chr1.merged.gds App Settings Output prefix: 1KG_phase3_subset_chr1 This will create a 1KG_phase3_subset_chr1_king_robust.gds file that contains the KING estimates as well as a 1KG_phase3_subset_chr1_king_robust_all.pdf with the plot of estimated kinship vs. IBS0. Use the “view stats and logs” button to check on the status of your tasks. Click on the bar that says “king_robust”, click “view logs”, and click the “king_robust” folder. In here you can see detailed logs of the job; take a look at the job.out.log and job.err.log – these can be useful for debugging issues. 10.2.2 PC-AiR Run the PC-AiR workflow to compute ancestry principal components (see the task “4. PC-AiR run”). Inputs Full GDS File: 1KG_phase3_subset_chr1.merged.gds Kinship File: 1KG_phase3_subset_chr1_king_robust.gds Phenotype File: sample_phenotype_pcs.RData Pruned GDS File: 1KG_phase3_subset_chr1.merged.gds App Settings Output prefix: 1KG_phase3_subset_chr1 Group: “Population” Run PC-variant correlation: FALSE This will create a 1KG_phase3_subset_chr1_pca.RData file that contains the output PCs as well as several PC plots. Take a look at the 1KG_phase3_subset_chr1_pca_parcoord.pdf – how many PCs to adjust for in PC-Relate and the association analyses? 10.2.3 PC-Relate Run the PC-Relate workflow to compute ancestry adjusted relatedness estimates (see the task “5. PC-Relate run”). Inputs GDS File: 1KG_phase3_subset_chr1.merged.gds PCA File: 1KG_phase3_subset_chr1_pca.RData App Settings Number of PCs: 2 Output prefix: 1KG_phase3_subset_chr1 This will create a 1KG_phase3_subset_chr1_pcrelate.RData file that contains the PC-Relate relatedness estimates as well as a 1KG_phase3_subset_chr1_pcrelate_all.pdf with the plot of estimated kinship vs. k0. (Note that the estimates are very noisy because we are using so few variants in this toy example.) From the “view stats and logs” page, click “instance metrics” to see an overview of CPU, RAM, etc. usage; this will update as the process runs. 10.3 Association Testing 10.3.1 Null Model Fit a null model using the Null Model workflow (see the task “6. GENESIS Null Model run”). Inputs Phenotype File: sample_phenotype_pcs.RData (note that the PCs are included in this file) Relatedness matrix file: kinship.RData App Settings Outcome: height Covariates: age, sex, study, PC1, PC2, PC3, PC4 Group variate: study Family: gaussian Output prefix: height This will create a height_null_model.RData file that contains the null model fit and a height_phenotypes.RData file that contains the phenotype data used in the analysis. It also creates a height_report.html null model report – review this report. 10.3.2 Single variant association test Use the GENESIS Single Variant Association Testing workflow to run a single variant association test (see the task “7. GENESIS Single Variant Association Testing run”). Inputs GDS files: 1KG_phase3_subset_chr1.merged.gds Null model file: height_null_model.RData Phenotype file: height_phenotypes.RData (it is recommended you use the file produced by the Null Model workflow) App Settings MAC threshold: 5 Output prefix: height_single memory GB: 32 This will create a height_single_chr1.RData file with the association test results as well as height_single_manh.png and height_single_qq.png files – review the QQ and Manhattan plots. 10.3.3 Aggregate variant test Use the GENESIS Aggregate Association Testing workflow to run a burden association test (see the task “8. GENESIS Aggregate Association Testing run”). Inputs GDS files: 1KG_phase3_subset_chr1.merged.gds Null model file: height_null_model.RData Phenotype file: height_phenotypes.RData (it is recommended you use the file produced by the Null Model workflow) Variant group files: variants_by_gene_chr1.RData App Settings Alt Freq Max: 0.1 Test: burden Output prefix: height_burden Memory GB: 32 This will create a height_burden_chr1.RData file with the association test results as well as height_burden_manh.png and height_burden_qq.png files – review the QQ and Manhattan plots. 10.4 Analysis follow-up In RStudio, locate the results of your association test under /sbgenomics/project-files/. Load one of these results files into R and explore it. "]]
